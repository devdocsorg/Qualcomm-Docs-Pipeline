<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Linux Target Device (Android / Linux Embedded / Ubuntu) &mdash; Qualcomm® AI Engine Direct</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/custom_css.css?v=22c3e01d" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=474e5199"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="CNN to QNN for Windows Host" href="qnn_tutorial_windows_1.html" />
    <link rel="prev" title="CNN to QNN for Linux Host" href="qnn_tutorial_linux_1.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Qualcomm® AI Engine Direct
          </a>
              <div class="version">
                2.24
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="tutorials.html">Tutorials</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="tutorials.html#getting-started">Getting Started</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="qnn_tutorial_linux_1.html">CNN to QNN for Linux Host</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="qnn_tutorial_linux_1.html#tutorial-setup">Tutorial Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="qnn_tutorial_linux_1.html#converting-the-cnn-model-into-a-qnn-model">Converting the CNN model into a QNN model</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="qnn_tutorial_linux_1.html#build-the-model-for-a-target-device">Build the Model for a Target Device</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="qnn_tutorial_windows_1.html">CNN to QNN for Windows Host</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="tutorials.html#advanced">Advanced</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorials.html#custom-operators">Custom Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorials.html#windows">Windows</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorials.html#migrating">Migrating</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Qualcomm® AI Engine Direct</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="tutorials.html">Tutorials</a></li>
          <li class="breadcrumb-item"><a href="qnn_tutorial_linux_1.html">CNN to QNN for Linux Host</a></li>
      <li class="breadcrumb-item active">Linux Target Device (Android / Linux Embedded / Ubuntu)</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="linux-target-device-android-linux-embedded-ubuntu">
<h1>Linux Target Device (Android / Linux Embedded / Ubuntu)<a class="headerlink" href="#linux-target-device-android-linux-embedded-ubuntu" title="Link to this heading"></a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is <strong>Part 2</strong> of the CNN to QNN tutorial for Windows host machines. If you have not completed Part 1, please do so <a class="reference internal" href="qnn_tutorial_linux_1.html"><span class="doc">here</span></a>.</p>
</div>
<section id="model-build-on-linux-host">
<h2>Model Build on Linux Host<a class="headerlink" href="#model-build-on-linux-host" title="Link to this heading"></a></h2>
<p>Once the CNN model has been converted into QNN format, the next step is to build it so it can run on the target device’s operating system with <code class="docutils literal notranslate"><span class="pre">qnn-model-lib-generator</span></code>.</p>
<p>Based on the operating system and architecture of your target device, choose one of the following build instructions.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>For cases where the “host machine” and “target device” are the same (e.g., you want to build and run model inferences on your Snapdragon for Windows device), you will need to adapt the steps to copy files to your current device instead of a remote device.</p>
</div>
<ol class="arabic">
<li><p>Create a directory on your host machine where your newly built files will live by running:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">mkdir</span><span class="w"> </span><span class="o">-</span><span class="n">p</span><span class="w"> </span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">qnn_tmp</span>
</pre></div>
</div>
</li>
<li><p>Navigate to the new directory:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span><span class="w"> </span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">qnn_tmp</span>
</pre></div>
</div>
</li>
<li><p>Copy over the QNN <cite>.cpp</cite> and <cite>.bin</cite> model files:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">Copy</span><span class="o">-</span><span class="n">Item</span><span class="w"> </span><span class="o">-</span><span class="n">Path</span><span class="w"> </span><span class="s">&quot;$env:QNN_SDK_ROOT\examples\Models\InceptionV3\model\Inception_v3.cpp&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;$env:QNN_SDK_ROOT\examples\Models\InceptionV3\model\Inception_v3.bin&quot;</span><span class="w"> </span><span class="o">-</span><span class="n">Destination</span><span class="w"> </span><span class="s">&quot;C:</span><span class="se">\t</span><span class="s">mp\qnn_tmp&quot;</span>
</pre></div>
</div>
</li>
<li><p>Choose the most relevant supported target architecture from the following list:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">clang</span>
<span class="n">aarch64</span><span class="o">-</span><span class="n">android</span>
<span class="n">windows</span><span class="o">-</span><span class="n">x86_64</span>
<span class="n">windows</span><span class="o">-</span><span class="n">aarch64</span>
<span class="n">aarch64</span><span class="o">-</span><span class="n">qnx</span>
<span class="n">aarch64</span><span class="o">-</span><span class="n">oe</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">gcc11</span><span class="mf">.2</span>
<span class="n">aarch64</span><span class="o">-</span><span class="n">oe</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">gcc9</span><span class="mf">.3</span>
<span class="n">aarch64</span><span class="o">-</span><span class="n">oe</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">gcc8</span><span class="mf">.2</span>
<span class="n">aarch64</span><span class="o">-</span><span class="n">ubuntu</span><span class="o">-</span><span class="n">gcc7</span><span class="mf">.5</span>
<span class="n">aarch64</span><span class="o">-</span><span class="n">ubuntu</span><span class="o">-</span><span class="n">gcc9</span><span class="mf">.4</span>
</pre></div>
</div>
</li>
<li><p>Choose the target architecture of your target device and set it as a console variable like so:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$QNN_TARGET_ARCH</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;aarch64-android&quot;</span>
</pre></div>
</div>
</li>
<li><p>Run the following command to generate the model library, updating the <cite>t</cite> value with the target architecture:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">python</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator&quot;</span><span class="w"> </span>\
<span class="w">    </span><span class="o">-</span><span class="n">c</span><span class="w"> </span><span class="s">&quot;Inception_v3.cpp&quot;</span><span class="w"> </span>\
<span class="w">    </span><span class="o">-</span><span class="n">b</span><span class="w"> </span><span class="s">&quot;Inception_v3.bin&quot;</span><span class="w"> </span>\
<span class="w">    </span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">model_libs</span><span class="w"> </span>\
<span class="w">    </span><span class="o">-</span><span class="n">t</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_TARGET_ARCH</span><span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
<ul class="simple">
<li><p><cite>c</cite> - This indicates the path to the <cite>.cpp</cite> QNN model file.</p></li>
<li><p><cite>b</cite> - This indicates the path to the <cite>.bin</cite> QNN model file. (<cite>b</cite> is optional, but at runtime, the <cite>.cpp</cite> file could fail if it needs the <cite>.bin</cite> file, so it is recommended).</p></li>
<li><p><cite>o</cite> - The path to the output folder.</p></li>
<li><p><cite>t</cite> - Indicate which architecture to build for.</p></li>
</ul>
<ol class="arabic simple" start="7">
<li><dl class="simple">
<dt>The output <cite>.so</cite> file will be inside <cite>model_libs</cite> with a name that corresponds to the target architecture specified with <cite>-t</cite> in the above command. For example:</dt><dd><ul class="simple">
<li><p><cite>model_libs/aarch64-android/libInception_v3.so</cite></p></li>
<li><p><cite>model_libs/aarch64-oe-linux-gcc11.2/libInception_v3.so</cite></p></li>
</ul>
</dd>
</dl>
</li>
</ol>
<p>That <cite>libInception_v3.so</cite> file is what you will use on the target device to actually execute inferences. See the below instructions for how to run inferences on the specific processors you care about using <cite>libInception_v3.so</cite>.</p>
</section>
<section id="optional-using-the-provided-setup-script">
<h2>Optional: Using the Provided Setup Script<a class="headerlink" href="#optional-using-the-provided-setup-script" title="Link to this heading"></a></h2>
<p>The above steps (model conversion &amp; model build) can be completed with the provided setup script. To convert and build the InceptionV3 model using the script, run:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">python</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">setup_inceptionv3</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="o">-</span><span class="n">a</span><span class="w"> </span><span class="o">~/</span><span class="n">tmpdir</span><span class="w"> </span><span class="o">-</span><span class="n">d</span><span class="w"> </span><span class="o">-</span><span class="n">c</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Passing in <cite>-q</cite> will produce a quantized model.</p>
</div>
<p>This will produce the same artifacts as above.</p>
</section>
<section id="use-the-built-model-on-specific-processors">
<h2>Use the built model on specific processors<a class="headerlink" href="#use-the-built-model-on-specific-processors" title="Link to this heading"></a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For accessing a remote target device running a Linux distro, we recommend using <cite>adb</cite>, but the high-level steps can be accomplished with any method of connecting, transferring files, and executing commands.</p>
</div>
<ol class="arabic">
<li><dl>
<dt>Install <cite>adb</cite> on your host machine if you have not already.</dt><dd><ul>
<li><p>Download the latest version of Android platform tools here: <cite>https://developer.android.com/tools/releases/platform-tools</cite></p></li>
<li><p>Extract the downloaded folder.</p></li>
<li><p>Replace the <cite>$adbParentFolderPath</cite> value with the path to the folder that contains <cite>adb</cite>, then run the command:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$adbParentFolderPath</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;C:\path</span><span class="se">\t</span><span class="s">o</span><span class="se">\f</span><span class="s">older</span><span class="se">\a</span><span class="s">bove</span><span class="se">\a</span><span class="s">db&quot;</span><span class="p">;</span><span class="w"> </span><span class="n">$currentPath</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">Environment</span><span class="p">]</span><span class="o">::</span><span class="n">GetEnvironmentVariable</span><span class="p">(</span><span class="s">&quot;Path&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;User&quot;</span><span class="p">);</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">$currentPath</span><span class="w"> </span><span class="o">-</span><span class="n">notcontains</span><span class="w"> </span><span class="n">$adbParentFolderPath</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="p">[</span><span class="n">Environment</span><span class="p">]</span><span class="o">::</span><span class="n">SetEnvironmentVariable</span><span class="p">(</span><span class="s">&quot;Path&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;$currentPath;$adbParentFolderPath &quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;User&quot;</span><span class="p">)</span><span class="w"> </span><span class="p">};</span><span class="w"> </span><span class="n">$env</span><span class="o">:</span><span class="n">Path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">System</span><span class="p">.</span><span class="n">Environment</span><span class="p">]</span><span class="o">::</span><span class="n">GetEnvironmentVariable</span><span class="p">(</span><span class="s">&quot;Path&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;User&quot;</span><span class="p">);</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="o">--</span><span class="n">version</span>
</pre></div>
</div>
</li>
<li><p>You should see something like <cite>Android Debug Bridge version 1.0.41</cite> proving that <cite>adb</cite> has been installed properly.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">&amp;</span> <span class="pre">ls</span> <span class="pre">${QNN_SDK_ROOT}/bin</span></code> to see the list of all supported target device architectures. (Each folder represents a different architecture)</p></li>
<li><p>Set the terminal variable <code class="docutils literal notranslate"><span class="pre">$QNN_TARGET_ARCH</span></code> to the name of your target device’s architecture:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$QNN_TARGET_ARCH</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;your-target-architecture (ex. aarch64-android)&quot;</span>
</pre></div>
</div>
</li>
<li><p>Pick the target device processor you want to run your built model on and follow the corresponding instructions below.</p></li>
</ol>
</section>
<section id="cpu-model-execution">
<h2>CPU Model Execution<a class="headerlink" href="#cpu-model-execution" title="Link to this heading"></a></h2>
<ol class="arabic">
<li><dl>
<dt>Connect to the target device and create a folder for the model files and input data (target specific).</dt><dd><ul>
<li><p>For an OE-Linux target device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">adb</span><span class="w"> </span><span class="n">shell</span><span class="w"> </span><span class="s">&quot;mount -o remount,rw / &amp;&amp; mkdir -p /data/local/tmp &amp;&amp; cd /data/local/tmp &amp;&amp; ln -s /etc/ /data/local/tmp &amp;&amp; chmod -R 777 /data/local/tmp &amp;&amp; mkdir -p /data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>For an android target:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">adb</span><span class="w"> </span><span class="n">shell</span><span class="w"> </span><span class="s">&quot;mkdir -p /data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
</ul>
</dd>
</dl>
</li>
<li><p>Push <cite>libQnnCpu.so</cite> and your built model (<cite>inception_v3.so</cite>) to your target device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/lib/${QNN_TARGET_ARCH}/libQnnCpu.so&quot;</span><span class="w"> </span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
<span class="o">&amp;</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/examples/Models/InceptionV3/model_libs/${QNN_TARGET_ARCH}/*.so&quot;</span><span class="w"> </span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Push the input data and input lists to your target device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped&quot;</span><span class="w"> </span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
<span class="o">&amp;</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/target_raw_list.txt&quot;</span><span class="w"> </span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Push the <cite>qnn-net-run</cite> tool which will actually execute the inferences:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="o">&amp;</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/bin/${QNN_TARGET_ARCH}/qnn-net-run /data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><dl class="simple">
<dt>Set up the environment on your target device by running:</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">adb</span> <span class="pre">shell</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">/data/local/tmp/inception_v3</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">LD_LIBRARY_PATH=/data/local/tmp/inception_v3</span></code></p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Use <cite>qnn-net-run</cite> in the target device shell to execute the inference on the example inputs:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="w"> </span><span class="o">--</span><span class="n">backend</span><span class="w"> </span><span class="n">libQnnCpu</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">libInception_v3</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">target_raw_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span><span class="o">--</span><span class="n">output</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">output</span>
</pre></div>
</div>
</li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">exit</span></code> to return to the host machine Powershell.</p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">“${QNN_SDK_ROOT}/examples/Models/InceptionV3”</span></code>.</p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">adb</span> <span class="pre">pull</span> <span class="pre">/data/local/tmp/inception_v3/output</span> <span class="pre">output</span></code> to extract the output from your target device to your host machine.</p></li>
<li><p>Run the following command to transform the output into a simple summary:</p>
<p>python ${QNN_SDK_ROOT}/examples/Models/InceptionV3/scripts/show_inceptionv3_classifications.py -i data/cropped/raw_list.txt -o output/ -l data/imagenet_slim_labels.txt</p>
</li>
<li><p>Verify that the classification results in <cite>output</cite> match the following:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">trash_bin</span><span class="p">.</span><span class="n">raw</span><span class="w">   </span><span class="mf">0.777344</span><span class="w"> </span><span class="mi">413</span><span class="w"> </span><span class="n">ashcan</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">chairs</span><span class="p">.</span><span class="n">raw</span><span class="w">      </span><span class="mf">0.253906</span><span class="w"> </span><span class="mi">832</span><span class="w"> </span><span class="n">studio</span><span class="w"> </span><span class="n">couch</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">plastic_cup</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span><span class="mf">0.980469</span><span class="w"> </span><span class="mi">648</span><span class="w"> </span><span class="n">measuring</span><span class="w"> </span><span class="n">cup</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">notice_sign</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span><span class="mf">0.167969</span><span class="w"> </span><span class="mi">459</span><span class="w"> </span><span class="n">brass</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="gpu-model-execution">
<h2>GPU Model Execution<a class="headerlink" href="#gpu-model-execution" title="Link to this heading"></a></h2>
<ol class="arabic">
<li><dl>
<dt>Connect to the target device and create a folder for the model files and input data (target specific).</dt><dd><ul>
<li><p>For an OE-Linux target device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">adb</span><span class="w"> </span><span class="n">shell</span><span class="w"> </span><span class="s">&quot;mount -o remount,rw / &amp;&amp; mkdir -p /data/local/tmp &amp;&amp; cd /data/local/tmp &amp;&amp; ln -s /etc/ /data/local/tmp &amp;&amp; chmod -R 777 /data/local/tmp &amp;&amp; mkdir -p /data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>For an Android target:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">adb</span><span class="w"> </span><span class="n">shell</span><span class="w"> </span><span class="s">&quot;mkdir -p /data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
</ul>
</dd>
</dl>
</li>
<li><p>Push <cite>libQnnGpu.so</cite> and your built model (<cite>inception_v3.so</cite>) to your target device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/lib/${QNN_TARGET_ARCH}/libQnnGpu.so&quot;</span><span class="w"> </span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
<span class="o">&amp;</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/examples/Models/InceptionV3/model_libs/${QNN_TARGET_ARCH}/*.so&quot;</span><span class="w"> </span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Push the input data and input lists to your target device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped&quot;</span><span class="w"> </span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
<span class="o">&amp;</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/target_raw_list.txt&quot;</span><span class="w"> </span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Push the <cite>qnn-net-run</cite> tool which will actually execute the inferences:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="o">&amp;</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/bin/${QNN_TARGET_ARCH}/qnn-net-run&quot;</span><span class="w"> </span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><dl class="simple">
<dt>Set up the environment on your target device by running:</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">adb</span> <span class="pre">shell</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">/data/local/tmp/inception_v3</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">LD_LIBRARY_PATH=/data/local/tmp/inception_v3</span></code></p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Use <cite>qnn-net-run</cite> in the target device shell to execute the inference on the example inputs:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="w"> </span><span class="o">--</span><span class="n">backend</span><span class="w"> </span><span class="n">libQnnGpu</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">libInception_v3</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">target_raw_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span><span class="o">--</span><span class="n">output</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">output</span>
</pre></div>
</div>
</li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">exit</span></code> to return to the host machine Powershell:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">exit</span>
<span class="n">cd</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/examples/Models/InceptionV3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">adb</span> <span class="pre">pull</span></code> to extract the output from your target device to your host machine:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">adb</span><span class="w"> </span><span class="n">pull</span><span class="w"> </span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">inception_v3</span><span class="o">/</span><span class="n">output</span><span class="w"> </span><span class="n">output</span>
</pre></div>
</div>
</li>
<li><p>Run the following command to transform the output into a simple summary:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">python3</span><span class="w"> </span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">show_inceptionv3_classifications</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="o">-</span><span class="n">i</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">raw_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span><span class="o">-</span><span class="n">o</span><span class="w"> </span><span class="n">output</span><span class="o">/</span><span class="w"> </span><span class="o">-</span><span class="n">l</span><span class="w"> </span><span class="n">data</span><span class="o">/</span><span class="n">imagenet_slim_labels</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
</li>
<li><p>Verify that the classification results in <cite>output</cite> match the following:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">trash_bin</span><span class="p">.</span><span class="n">raw</span><span class="w">   </span><span class="mf">0.777344</span><span class="w"> </span><span class="mi">413</span><span class="w"> </span><span class="n">ashcan</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">chairs</span><span class="p">.</span><span class="n">raw</span><span class="w">      </span><span class="mf">0.253906</span><span class="w"> </span><span class="mi">832</span><span class="w"> </span><span class="n">studio</span><span class="w"> </span><span class="n">couch</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">plastic_cup</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span><span class="mf">0.980469</span><span class="w"> </span><span class="mi">648</span><span class="w"> </span><span class="n">measuring</span><span class="w"> </span><span class="n">cup</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">notice_sign</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span><span class="mf">0.167969</span><span class="w"> </span><span class="mi">459</span><span class="w"> </span><span class="n">brass</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="dsp-model-execution">
<h2>DSP Model Execution<a class="headerlink" href="#dsp-model-execution" title="Link to this heading"></a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DSP processors require quantized models instead of floating point models. If you do not have a quantized model, please follow Step 1 of the CNN to QNN tutorial to build one.</p>
</div>
<ol class="arabic">
<li><dl>
<dt>Connect to the target device and create a folder for the model files and input data (target specific).</dt><dd><ul>
<li><p>For an OE-Linux target device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">adb</span><span class="w"> </span><span class="n">shell</span><span class="w"> </span><span class="s">&quot;mount -o remount,rw / &amp;&amp; mkdir -p /data/local/tmp &amp;&amp; cd /data/local/tmp &amp;&amp; ln -s /etc/ /data/local/tmp &amp;&amp; chmod -R 777 /data/local/tmp &amp;&amp; mkdir -p /data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>For an Android target:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">adb</span><span class="w"> </span><span class="n">shell</span><span class="w"> </span><span class="s">&quot;mkdir -p /data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
</ul>
</dd>
</dl>
</li>
<li><p>Look up your target device’s Snapdragon architecture in this “Supported Snapdragon Devices” table and set <code class="docutils literal notranslate"><span class="pre">$DSP_ARCH</span></code> to <cite>hexagon-vXX</cite> where XX is the version of your Hexagon Architecture. For example:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$DSP_ARCH</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;hexagon-v68&quot;</span>
</pre></div>
</div>
</li>
<li><p>Push <cite>libQnnDsp.so</cite> and your built model (<cite>libInception_v3.so</cite>) to your target device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/lib/${QNN_TARGET_ARCH}/libQnnDsp.so&quot;</span><span class="w"> </span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
<span class="o">&amp;</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/examples/Models/InceptionV3/model_libs/${QNN_TARGET_ARCH}/*&quot;</span><span class="w"> </span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Push the specific versions of your <code class="docutils literal notranslate"><span class="pre">$DSP_ARCH</span></code> files by replacing <cite>libQnnDspV66Stub.so</cite> and <cite>libQnnDspV66Stub.so</cite> with your version (e.g., <cite>libQnnDspV69Stub.so</cite> for v69):</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="o">&amp;</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/lib/${DSP_ARCH}/unsigned/libQnnDspV66Skel.so&quot;</span><span class="w"> </span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
<span class="o">&amp;</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/lib/${QNN_TARGET_ARCH}/libQnnDspV66Stub.so&quot;</span><span class="w"> </span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Push the input data and input lists to your target device:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped&quot;</span><span class="w"> </span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
<span class="o">&amp;</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/target_raw_list.txt&quot;</span><span class="w"> </span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Push the <cite>qnn-net-run</cite> tool which will actually execute the inferences:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="o">&amp;</span><span class="w"> </span><span class="n">adb</span><span class="w"> </span><span class="n">push</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/bin/${QNN_TARGET_ARCH}/qnn-net-run&quot;</span><span class="w"> </span><span class="s">&quot;/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><dl class="simple">
<dt>Set up the environment on your target device by running:</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">adb</span> <span class="pre">shell</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">/data/local/tmp/inception_v3</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">VENDOR_LIB=/vendor/lib/</span></code>  # /vendor/lib64/ if aarch64</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">LD_LIBRARY_PATH=/data/local/tmp/inception_v3:/vendor/dsp/cdsp:$VENDOR_LIB</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">ADSP_LIBRARY_PATH=&quot;/data/local/tmp/inception_v3;/vendor/dsp/cdsp;/vendor/lib/rfsa/adsp;/system/lib/rfsa/adsp;/dsp&quot;</span></code></p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Use <cite>qnn-net-run</cite> in the target device shell to execute the inference on the example inputs:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">.</span><span class="o">/</span><span class="n">qnn</span><span class="o">-</span><span class="n">net</span><span class="o">-</span><span class="n">run</span><span class="w"> </span><span class="o">--</span><span class="n">backend</span><span class="w"> </span><span class="n">libQnnDsp</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">--</span><span class="n">model</span><span class="w"> </span><span class="n">libInception_v3</span><span class="p">.</span><span class="n">so</span><span class="w"> </span><span class="o">--</span><span class="n">input_list</span><span class="w"> </span><span class="n">target_raw_list</span><span class="p">.</span><span class="n">txt</span><span class="w"> </span><span class="o">--</span><span class="n">output_dir</span><span class="w"> </span><span class="n">output</span>
</pre></div>
</div>
</li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">exit</span></code> to return to the host machine Powershell:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">exit</span>
<span class="n">cd</span><span class="w"> </span><span class="s">&quot;${QNN_SDK_ROOT}/examples/Models/InceptionV3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">adb</span> <span class="pre">pull</span></code> to extract the output from your target device to your host machine:</p>
<p>adb pull /data/local/tmp/inception_v3/output output</p>
</li>
<li><p>Run the following command to transform the output into a simple summary:</p>
<p>python3 ${QNN_SDK_ROOT}/examples/Models/InceptionV3/scripts/show_inceptionv3_classifications.py -i data/cropped/raw_list.txt -o output/ -l data/imagenet_slim_labels.txt</p>
</li>
<li><p>Verify that the classification results in <cite>output</cite> match the following:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">trash_bin</span><span class="p">.</span><span class="n">raw</span><span class="w">   </span><span class="mf">0.777344</span><span class="w"> </span><span class="mi">413</span><span class="w"> </span><span class="n">ashcan</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">chairs</span><span class="p">.</span><span class="n">raw</span><span class="w">      </span><span class="mf">0.253906</span><span class="w"> </span><span class="mi">832</span><span class="w"> </span><span class="n">studio</span><span class="w"> </span><span class="n">couch</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">plastic_cup</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span><span class="mf">0.980469</span><span class="w"> </span><span class="mi">648</span><span class="w"> </span><span class="n">measuring</span><span class="w"> </span><span class="n">cup</span>
<span class="n">$</span><span class="p">{</span><span class="n">QNN_SDK_ROOT</span><span class="p">}</span><span class="o">/</span><span class="n">examples</span><span class="o">/</span><span class="n">Models</span><span class="o">/</span><span class="n">InceptionV3</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">cropped</span><span class="o">/</span><span class="n">notice_sign</span><span class="p">.</span><span class="n">raw</span><span class="w"> </span><span class="mf">0.167969</span><span class="w"> </span><span class="mi">459</span><span class="w"> </span><span class="n">brass</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="htp-model-execution">
<h2>HTP Model Execution<a class="headerlink" href="#htp-model-execution" title="Link to this heading"></a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>HTP processors require quantized models instead of floating point models. If you do not have a quantized model, please follow Step 1 of the CNN to QNN tutorial to build one.</p>
</div>
<p><strong>Additional HTP Required Setup</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Running the model on a target device’s HTP requires the generation of a serialized context.</p>
</div>
<ol class="arabic simple">
<li><p>Users can set the custom options and different performance modes to HTP Backend through the backend config. Please refer to <a class="reference external" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/htp_backend.html">QNN HTP Backend Extensions</a> for various options available in the config.</p></li>
<li><dl class="simple">
<dt>Refer to the example below for creating a backend config file for the QCS6490/QCM6490 target with mandatory options to be passed:</dt><dd><ol class="arabic simple">
<li><p>Update the following information based on your device’s <code class="docutils literal notranslate"><span class="pre">htp_arch</span></code>.</p></li>
</ol>
</dd>
</dl>
</li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="o">{</span>
<span class="w">    </span><span class="s2">&quot;graphs&quot;</span>:<span class="w"> </span><span class="o">[</span>
<span class="w">        </span><span class="o">{</span>
<span class="w">            </span><span class="s2">&quot;graph_names&quot;</span>:<span class="w"> </span><span class="o">[</span>
<span class="w">                </span><span class="s2">&quot;Inception_v3&quot;</span>
<span class="w">            </span><span class="o">]</span>,
<span class="w">            </span><span class="s2">&quot;vtcm_mb&quot;</span>:<span class="w"> </span><span class="m">2</span>
<span class="w">        </span><span class="o">}</span>
<span class="w">    </span><span class="o">]</span>,
<span class="w">    </span><span class="s2">&quot;devices&quot;</span>:<span class="w"> </span><span class="o">[</span>
<span class="w">        </span><span class="o">{</span>
<span class="w">            </span><span class="s2">&quot;htp_arch&quot;</span>:<span class="w"> </span><span class="s2">&quot;v68&quot;</span>
<span class="w">        </span><span class="o">}</span>
<span class="w">    </span><span class="o">]</span>
<span class="o">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>The above config file with minimum parameters such as backend extensions config specified through JSON is given below:</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="o">{</span>
<span class="w">    </span><span class="s2">&quot;backend_extensions&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">        </span><span class="s2">&quot;shared_library_path&quot;</span>:<span class="w"> </span><span class="s2">&quot;path_to_shared_library&quot;</span>,<span class="w">  </span>//<span class="w"> </span>give<span class="w"> </span>path<span class="w"> </span>to<span class="w"> </span>shared<span class="w"> </span>extensions<span class="w"> </span>library<span class="w"> </span><span class="o">(</span>.so<span class="o">)</span>
<span class="w">        </span><span class="s2">&quot;config_file_path&quot;</span>:<span class="w"> </span><span class="s2">&quot;path_to_config_file&quot;</span><span class="w">         </span>//<span class="w"> </span>give<span class="w"> </span>path<span class="w"> </span>to<span class="w"> </span>backend<span class="w"> </span>config
<span class="w">    </span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>To generate the context, update <code class="docutils literal notranslate"><span class="pre">&lt;path</span> <span class="pre">to</span> <span class="pre">JSON</span> <span class="pre">of</span> <span class="pre">backend</span> <span class="pre">extensions&gt;</span></code> below with the config you wrote above, then run the command:</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="p">&amp;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/bin/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/qnn-context-binary-generator&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>--backend<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/libQnnHtp.so&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>--model<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/model_libs/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/libInception_v3.so&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>--binary_file<span class="w"> </span><span class="s2">&quot;Inception_v3.serialized&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>--config_file<span class="w"> </span>&lt;path<span class="w"> </span>to<span class="w"> </span>JSON<span class="w"> </span>of<span class="w"> </span>backend<span class="w"> </span>extensions&gt;
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><dl class="simple">
<dt>This creates the serialized context at:</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/output/Inception_v3.serialized.bin</span></code></p></li>
</ul>
</dd>
</dl>
</li>
</ol>
<p><strong>Running the built model</strong></p>
<ol class="arabic">
<li><dl>
<dt>Connect to the target device and create a folder for the model files and input data (target specific).</dt><dd><ol class="arabic">
<li><p>For an OE-Linux target device:</p>
<blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>adb<span class="w"> </span>shell<span class="w"> </span><span class="s2">&quot;mount -o remount,rw / &amp;&amp; mkdir -p /data/local/tmp &amp;&amp; cd /data/local/tmp &amp;&amp; ln -s /etc/ /data/local/tmp &amp;&amp; chmod -R 777 /data/local/tmp &amp;&amp; mkdir -p /data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>For an Android target:</p>
<blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>adb<span class="w"> </span>shell<span class="w"> </span><span class="s2">&quot;mkdir -p /data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ol>
</dd>
</dl>
</li>
<li><p>Look up your target device’s snapdragon architecture in this “Supported Snapdragon Devices” table and set <code class="docutils literal notranslate"><span class="pre">$HTP_ARCH</span></code> to <code class="docutils literal notranslate"><span class="pre">hexagon-vXX</span></code> where XX is the version of your Hexagon Architecture. For example:</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">$HTP_ARCH</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;hexagon-v68&quot;</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><dl class="simple">
<dt>Push <code class="docutils literal notranslate"><span class="pre">libQnnHtp.so</span></code> and your built model (<code class="docutils literal notranslate"><span class="pre">Inception_v3.serialized.bin</span></code>) to your target device:</dt><dd><ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&amp;</span> <span class="pre">adb</span> <span class="pre">push</span> <span class="pre">&quot;${QNN_SDK_ROOT}/lib/${HTP_ARCH}/unsigned/*&quot;</span> <span class="pre">&quot;/data/local/tmp/inception_v3&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&amp;</span> <span class="pre">adb</span> <span class="pre">push</span> <span class="pre">&quot;${QNN_SDK_ROOT}/lib/${QNN_TARGET_ARCH}/libQnnHtp.so&quot;</span> <span class="pre">&quot;/data/local/tmp/inception_v3&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&amp;</span> <span class="pre">adb</span> <span class="pre">push</span> <span class="pre">&quot;${QNN_SDK_ROOT}/examples/Models/InceptionV3/output/Inception_v3.serialized.bin&quot;</span> <span class="pre">&quot;/data/local/tmp/inception_v3&quot;</span></code></p></li>
</ol>
</dd>
</dl>
</li>
<li><p>Push the specific version of your <code class="docutils literal notranslate"><span class="pre">$HTP_ARCH</span></code> file by replacing <code class="docutils literal notranslate"><span class="pre">libQnnHtpV68Stub.so</span></code> with your version (ex. <code class="docutils literal notranslate"><span class="pre">libQnnHtpV69Stub.so</span></code> for v69:</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="p">&amp;</span><span class="w"> </span>adb<span class="w"> </span>push<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/libQnnHtpV68Stub.so&quot;</span><span class="w"> </span><span class="s2">&quot;/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>Push the input data and input lists to your target device:</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>adb<span class="w"> </span>push<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/data/cropped&quot;</span><span class="w"> </span><span class="s2">&quot;/data/local/tmp/inception_v3&quot;</span>
<span class="p">&amp;</span><span class="w"> </span>adb<span class="w"> </span>push<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/data/target_raw_list.txt&quot;</span><span class="w"> </span><span class="s2">&quot;/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
<ol class="arabic simple" start="6">
<li><p>Push the <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> tool which will actually execute the inferences:</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="p">&amp;</span><span class="w"> </span>adb<span class="w"> </span>push<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/bin/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/qnn-net-run&quot;</span><span class="w"> </span><span class="s2">&quot;/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
<ol class="arabic simple" start="7">
<li><dl class="simple">
<dt>Set up the environment on your target device by running:</dt><dd><ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">adb</span> <span class="pre">shell</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">/data/local/tmp/inception_v3</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">LD_LIBRARY_PATH=/data/local/tmp/inception_v3</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">ADSP_LIBRARY_PATH=&quot;/data/local/tmp/inception_v3&quot;</span></code></p></li>
</ol>
</dd>
</dl>
</li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> in the target device shell to execute the inference on the example inputs:</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./qnn-net-run<span class="w"> </span>--backend<span class="w"> </span>libQnnHtp.so<span class="w"> </span>--input_list<span class="w"> </span>target_raw_list.txt<span class="w"> </span>--retrieve_context<span class="w"> </span>Inception_v3.serialized.bin<span class="w"> </span>--output<span class="w"> </span>./output
</pre></div>
</div>
<ol class="arabic simple" start="9">
<li><p>Run <code class="docutils literal notranslate"><span class="pre">exit</span></code> to return to the host machine Powershell:</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">exit</span>
<span class="nb">cd</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3&quot;</span>
</pre></div>
</div>
<ol class="arabic simple" start="10">
<li><p>Run <code class="docutils literal notranslate"><span class="pre">adb</span> <span class="pre">pull</span></code> to extract the output from your target device to your host machine:</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>adb<span class="w"> </span>pull<span class="w"> </span>/data/local/tmp/inception_v3/output<span class="w"> </span>output
</pre></div>
</div>
<ol class="arabic simple" start="11">
<li><p>Run the following command to transform the output into a simple summary:</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/examples/Models/InceptionV3/scripts/show_inceptionv3_classifications.py<span class="w"> </span>-i<span class="w"> </span>data/cropped/raw_list.txt<span class="w"> </span>-o<span class="w"> </span>output/<span class="w"> </span>-l<span class="w"> </span>data/imagenet_slim_labels.txt
</pre></div>
</div>
<ol class="arabic simple" start="12">
<li><p>Verify that the classification results in <code class="docutils literal notranslate"><span class="pre">output</span></code> match the following:</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/examples/Models/InceptionV3/data/cropped/trash_bin.raw<span class="w">   </span><span class="m">0</span>.777344<span class="w"> </span><span class="m">413</span><span class="w"> </span>ashcan
<span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/examples/Models/InceptionV3/data/cropped/chairs.raw<span class="w">      </span><span class="m">0</span>.253906<span class="w"> </span><span class="m">832</span><span class="w"> </span>studio<span class="w"> </span>couch
<span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/examples/Models/InceptionV3/data/cropped/plastic_cup.raw<span class="w"> </span><span class="m">0</span>.980469<span class="w"> </span><span class="m">648</span><span class="w"> </span>measuring<span class="w"> </span>cup
<span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/examples/Models/InceptionV3/data/cropped/notice_sign.raw<span class="w"> </span><span class="m">0</span>.167969<span class="w"> </span><span class="m">459</span><span class="w"> </span>brass
</pre></div>
</div>
</section>
<section id="low-power-ai-lpai-model-execution">
<h2>Low Power AI (LPAI) Model Execution<a class="headerlink" href="#low-power-ai-lpai-model-execution" title="Link to this heading"></a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>LPAI processors are designed for <strong>offline model preparation only</strong>. This means you must build the model first, then transfer it.</p>
</div>
<p>The offline generated model must be executed via the <a class="reference external" href="https://qpm.qualcomm.com/#/main/tools/details/LPAI">LPAI SDK</a>. You will have to sign in to access the LPAI SDK, and it may be dependent on filling out specific paperwork for your account to access the SDK.</p>
<p>You can use the following example files with the LPAI SDK to help run an inference on that processor:</p>
<p>EXAMPLE of <code class="docutils literal notranslate"><span class="pre">config.json</span></code> file:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="o">{</span>
<span class="w">   </span><span class="s2">&quot;backend_extensions&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">   </span><span class="s2">&quot;shared_library_path&quot;</span>:<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/lib/x86_64-linux-clang/libQnnLpaiNetRunExtentions.so&quot;</span>,
<span class="w">   </span><span class="s2">&quot;config_file_path&quot;</span>:<span class="w"> </span><span class="s2">&quot;./lpaiParams.conf&quot;</span>
<span class="w">   </span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<p>EXAMPLE of <code class="docutils literal notranslate"><span class="pre">lpaiParams.conf</span></code> file:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="o">{</span>
<span class="w">   </span><span class="s2">&quot;lpai_backend&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">      </span><span class="s2">&quot;target_env&quot;</span>:<span class="w"> </span><span class="s2">&quot;x86&quot;</span>,
<span class="w">      </span><span class="s2">&quot;enable_hw_ver&quot;</span>:<span class="w"> </span><span class="s2">&quot;v4&quot;</span>
<span class="w">   </span><span class="o">}</span>,
<span class="w">   </span><span class="s2">&quot;lpai_graph&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">      </span><span class="s2">&quot;prepare&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">         </span><span class="s2">&quot;enable_layer_fusion&quot;</span>:<span class="w"> </span>true,
<span class="w">         </span><span class="s2">&quot;enable_batchnorm_fold&quot;</span>:<span class="w"> </span>false,
<span class="w">         </span><span class="s2">&quot;enable_channel_align&quot;</span>:<span class="w"> </span>false,
<span class="w">         </span><span class="s2">&quot;pad_split&quot;</span>:<span class="w"> </span>false,
<span class="w">         </span><span class="s2">&quot;exclude_io&quot;</span>:<span class="w"> </span><span class="nb">false</span>
<span class="w">      </span><span class="o">}</span>,
<span class="w">      </span><span class="s2">&quot;graph_name_0&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">         </span><span class="s2">&quot;prepare&quot;</span>:<span class="w"> </span><span class="o">{</span>
<span class="w">            </span><span class="s2">&quot;enable_batchnorm_fold&quot;</span>:<span class="w"> </span><span class="nb">true</span>
<span class="w">         </span><span class="o">}</span>
<span class="w">      </span><span class="o">}</span>
<span class="w">   </span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<p>To configure <code class="docutils literal notranslate"><span class="pre">lpaiParams.conf</span></code>, consider using the following optional settings:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>lpai_backend
<span class="w">   </span><span class="s2">&quot;target_env&quot;</span><span class="w">              </span><span class="s2">&quot;arm/adsp/x86/tensilica, default adsp&quot;</span>
<span class="w">   </span><span class="s2">&quot;enable_hw_ver&quot;</span><span class="w">           </span><span class="s2">&quot;v1/v2/v3/v4, default v4&quot;</span>
lpai_graph
<span class="w">   </span>prepare
<span class="w">      </span><span class="s2">&quot;enable_layer_fusion&quot;</span><span class="w">     </span><span class="s2">&quot;true/false,     default true&quot;</span>
<span class="w">      </span><span class="s2">&quot;enable_batchnorm_fold&quot;</span><span class="w">   </span><span class="s2">&quot;true/false,     default false&quot;</span>
<span class="w">      </span><span class="s2">&quot;enable_channel_align&quot;</span><span class="w">    </span><span class="s2">&quot;true/false,     default false&quot;</span>
<span class="w">      </span><span class="s2">&quot;pad_split&quot;</span><span class="w">               </span><span class="s2">&quot;true/false,     default false&quot;</span>
<span class="w">      </span><span class="s2">&quot;exclude_io&quot;</span><span class="w">              </span><span class="s2">&quot;true/false,     default false&quot;</span>
</pre></div>
</div>
<p>Using the above <code class="docutils literal notranslate"><span class="pre">config.json</span></code> and <code class="docutils literal notranslate"><span class="pre">lpaiParams.conf</span></code> you can use <code class="docutils literal notranslate"><span class="pre">qnn-context-binary-generator</span></code> to build the LPAI offline model.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/examples/QNN/converter/models
<span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:<span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/lib/x86_64-linux-clang
<span class="o">&amp;&amp;</span><span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/bin/x86_64-linux-clang/qnn-context-binary-generator<span class="w"> </span><span class="se">\</span>
<span class="w">              </span>--backend<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/lib/x86_64-linux-clang/libQnnLpai.so<span class="w"> </span><span class="se">\</span>
<span class="w">              </span>--model<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/examples/Models/InceptionV3/model_libs/x86_64-linux-clang/&lt;libQnnModel.so&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">              </span>--config_file<span class="w"> </span>&lt;config.json&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">              </span>--log_level<span class="w"> </span>verbose<span class="w"> </span><span class="se">\</span>
<span class="w">              </span>--backend_binary<span class="w"> </span>&lt;output_graph.eai&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">              </span>--binary_file<span class="w"> </span>tmp
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="qnn_tutorial_linux_1.html" class="btn btn-neutral float-left" title="CNN to QNN for Linux Host" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="qnn_tutorial_windows_1.html" class="btn btn-neutral float-right" title="CNN to QNN for Windows Host" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2024, Qualcomm Technologies, Inc..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>