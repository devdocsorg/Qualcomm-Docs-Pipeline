<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Overview &mdash; Qualcomm® AI Engine Direct</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/custom_css.css?v=b1dec89a" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=474e5199"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tutorials" href="tutorials.html" />
    <link rel="prev" title="Introduction" href="introduction.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Qualcomm® AI Engine Direct
          </a>
              <div class="version">
                2.24
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#features">Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="#supported-snapdragon-devices">Supported Snapdragon devices</a></li>
<li class="toctree-l2"><a class="reference internal" href="#software-architecture">Software architecture</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#device">Device</a></li>
<li class="toctree-l3"><a class="reference internal" href="#backend">Backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="#context">Context</a></li>
<li class="toctree-l3"><a class="reference internal" href="#graph">Graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="#operation-package-registry">Operation Package registry</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#integration-workflow">Integration workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="#developers-on-linux">Developers on Linux</a></li>
<li class="toctree-l2"><a class="reference internal" href="#integration-workflow-on-windows">Integration workflow on Windows</a></li>
<li class="toctree-l2"><a class="reference internal" href="#developers-on-windows">Developers on Windows</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#wsl-platform">WSL platform</a></li>
<li class="toctree-l3"><a class="reference internal" href="#windows-platform">Windows platform</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Qualcomm® AI Engine Direct</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Overview</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="overview">
<h1>Overview<a class="headerlink" href="#overview" title="Link to this heading"></a></h1>
<p>Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> is the Qualcomm Technologies Inc. (QTI) software architecture for AI/ML use cases
on QTI chipsets and AI acceleration cores.</p>
<p>The Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> architecture is designed to provide a unified API and modular and extensible
per-accelerator libraries, which form a reusable basis for full stack AI solutions,
both for QTI’s own and third-party frameworks (shown in <a class="reference internal" href="#qnn-sw-stack-figure"><span class="std std-ref">AI Software Stack with Qualcomm AI Engine Direct</span></a>).</p>
<p class="centered" id="qnn-sw-stack-figure">
<strong><strong>AI Software Stack with Qualcomm AI Engine Direct</strong></strong></p><figure class="align-center">
<img alt="QNN software stack" src="../_images/qnn_software_stack.png" />
</figure>
<section id="features">
<h2>Features<a class="headerlink" href="#features" title="Link to this heading"></a></h2>
<p><strong>Modularity based on hardware accelerators</strong></p>
<p>The Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> architecture is designed to be modular and allows for clean separation in the software
for different hardware cores/accelerators, such as the CPU, GPU, and DSP that are designated as
<em>backends</em>.</p>
<p>Learn more about the Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> backends <span class="xref std std-doc">here</span>.</p>
<p>The Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> backends for different hardware cores/accelerators are compiled into
individual core-specific libraries that are packaged with the SDK.</p>
<p><strong>Unified API across IP Cores</strong></p>
<p>One of the key highlights of Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> is that it provides a unified API to delegate operations,
such as graph creation and execution across all hardware accelerator backends. This allows users
to treat Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> as a hardware abstraction API and port applications easily to different cores.</p>
<p><strong>Right level of abstraction</strong></p>
<p>The Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> API is designed to support an efficient execution model
with capabilities, such as graph optimizations, to be taken care of internally.
At the same time however, it leaves out broader functionality, such as model parsing and
network partitioning to higher level frameworks.</p>
<p><strong>Flexibility in composition</strong></p>
<p>With Qualcomm® <span class="xref std std-ref">AI Engine Direct</span>, users can choose appropriate tradeoffs between capabilities provided by the backends
and the footprint in terms of library size and memory utilization. This offers the ability to
compose a Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> Operation Package with only operations required to serve a set of models
targeted by a use case <a class="footnote-reference brackets" href="#id3" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>. With this, users can create nimble applications with
low-memory footprint that fits a wide variety of hardware products.</p>
<p><strong>Extensible operation support</strong></p>
<p>Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> also provides support for clients to integrate custom operations to work seamlessly alongside
the built-in operations.</p>
<p><strong>Improved execution performance</strong></p>
<p>With optimized network loading and asynchronous execution support, Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> serves to provide a highly
efficient interface for ML frameworks and applications to load and execute network graphs on their preferred hardware accelerator.</p>
</section>
<section id="supported-snapdragon-devices">
<h2>Supported Snapdragon devices<a class="headerlink" href="#supported-snapdragon-devices" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Snapdragon Device/Chip</p></th>
<th class="head"><p>Supported Toolchains</p></th>
<th class="head"><p>SOC Model <a class="footnote-reference brackets" href="#id4" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a></p></th>
<th class="head"><p>Hexagon Arch</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Snapdragon 8cx Gen 4 (SC8380XP)</p></td>
<td><p>aarch64-windows-msvc</p>
<p>arm64x-windows-msvc</p>
</td>
<td><p>60</p></td>
<td><p>V73</p></td>
</tr>
<tr class="row-odd"><td><p>Snapdragon 8cx Gen 3 (SC8280X)</p></td>
<td><p>aarch64-windows-msvc</p></td>
<td><p>37</p></td>
<td><p>V68</p></td>
</tr>
<tr class="row-even"><td><p>Snapdragon 7c Gen 2 (SC7280X)</p></td>
<td><p>aarch64-windows-msvc</p></td>
<td><p>44</p></td>
<td><p>V68</p></td>
</tr>
<tr class="row-odd"><td><p>SD 8 Gen 3 (SM8650)</p></td>
<td><p>aarch64-android</p></td>
<td><p>57</p></td>
<td><p>V75</p></td>
</tr>
<tr class="row-even"><td><p>SD 8 Gen 2 (SM8550)</p></td>
<td><p>aarch64-android</p></td>
<td><p>43</p></td>
<td><p>V73</p></td>
</tr>
<tr class="row-odd"><td><p>SD 8+ Gen 1 (SM8475)</p></td>
<td><p>aarch64-android</p></td>
<td><p>42</p></td>
<td><p>V69</p></td>
</tr>
<tr class="row-even"><td><p>SD 8 Gen 1 (SM8450)</p></td>
<td><p>aarch64-android</p></td>
<td><p>36</p></td>
<td><p>V69</p></td>
</tr>
<tr class="row-odd"><td><p>888+ (SM8350P)</p>
<p>888 (SM8350)</p>
</td>
<td><p>aarch64-android</p></td>
<td><p>30</p></td>
<td><p>V68</p></td>
</tr>
<tr class="row-even"><td><p>7 Gen 1 (SM7450)</p></td>
<td><p>aarch64-android</p></td>
<td><p>41</p></td>
<td><p>V69</p></td>
</tr>
<tr class="row-odd"><td><p>778G (SM7325)</p></td>
<td><p>aarch64-android</p></td>
<td><p>35</p></td>
<td><p>V68</p></td>
</tr>
<tr class="row-even"><td><p>QCM6490</p></td>
<td><p>aarch64-android</p>
<p>aarch64-ubuntu-gcc9.4</p>
<p>aarch64-oe-linux-gcc11.2</p>
</td>
<td><p>35</p></td>
<td><p>V68</p></td>
</tr>
<tr class="row-odd"><td><p>865 (SM8250)</p></td>
<td><p>aarch64-android</p></td>
<td><p>21</p></td>
<td><p>V66</p></td>
</tr>
<tr class="row-even"><td><p>765 (SM7250)</p></td>
<td><p>aarch64-android</p></td>
<td><p>25</p></td>
<td><p>V66</p></td>
</tr>
<tr class="row-odd"><td><p>750G (SM7225)</p>
<p>690 (SM6350)</p>
</td>
<td><p>aarch64-android</p></td>
<td><p>29</p></td>
<td><p>V66</p>
<p>V66</p>
</td>
</tr>
<tr class="row-even"><td><p>QRB5165</p></td>
<td><p>aarch64-ubuntu-gcc7.5</p>
<p>aarch64-oe-linux-gcc9.3</p>
</td>
<td><p>21</p></td>
<td><p>V66</p></td>
</tr>
<tr class="row-odd"><td><p>QCS7230</p></td>
<td><p>aarch64-android</p>
<p>aarch64-oe-linux-gcc9.3</p>
</td>
<td><p>51</p></td>
<td><p>V66</p></td>
</tr>
<tr class="row-even"><td><p>680 (SM6225)</p></td>
<td><p>aarch64-android</p></td>
<td><p>40</p></td>
<td><p>V66</p></td>
</tr>
<tr class="row-odd"><td><p>480 (SM4350)</p>
<p>695 (SM6375)</p>
</td>
<td><p>aarch64-android</p></td>
<td><p>31</p></td>
<td><p>V66</p>
<p>V66</p>
</td>
</tr>
<tr class="row-even"><td><p>460 (SM4250)</p>
<p>662 (SM6115)</p>
<p>QCM4290</p>
</td>
<td><p>aarch64-android</p></td>
<td><p>28</p></td>
<td><p>V66</p>
<p>V66</p>
<p>V66</p>
</td>
</tr>
<tr class="row-odd"><td><p>QCS610</p></td>
<td><p>aarch64-android</p>
<p>aarch64-oe-linux-gcc9.3</p>
</td>
<td><p>16</p></td>
<td><p>V66</p></td>
</tr>
<tr class="row-even"><td><p>QCS410</p></td>
<td><p>aarch64-android</p>
<p>aarch64-oe-linux-gcc9.3</p>
</td>
<td><p>33</p></td>
<td><p>V66</p></td>
</tr>
<tr class="row-odd"><td><p>QCM6125</p></td>
<td><p>aarch64-android</p></td>
<td><p>19</p></td>
<td><p>V66</p></td>
</tr>
<tr class="row-even"><td><p>QRB4210</p></td>
<td><p>aarch64-oe-linux-gcc9.3</p></td>
<td><p>49</p></td>
<td><p>V66</p></td>
</tr>
<tr class="row-odd"><td><p>QCM4490</p></td>
<td><p>aarch64-android</p></td>
<td><p>59</p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-even"><td><p>780G (SM7350)</p></td>
<td><p>aarch64-android</p></td>
<td><p>32</p></td>
<td><p>V68</p></td>
</tr>
<tr class="row-odd"><td><p>SM8325</p></td>
<td><p>aarch64-android</p></td>
<td><p>34</p></td>
<td><p>V68</p></td>
</tr>
<tr class="row-even"><td><p>SM7315</p></td>
<td><p>aarch64-android</p></td>
<td><p>38</p></td>
<td><p>V68</p></td>
</tr>
<tr class="row-odd"><td><p>6 Gen 1 (SM6450)</p></td>
<td><p>aarch64-android</p></td>
<td><p>50</p></td>
<td><p>V73</p></td>
</tr>
<tr class="row-even"><td><p>7+ Gen 2 (SM7475)</p></td>
<td><p>aarch64-android</p></td>
<td><p>54</p></td>
<td><p>V69</p></td>
</tr>
<tr class="row-odd"><td><p>4 Gen 2 (SM4450)</p></td>
<td><p>aarch64-android</p></td>
<td><p>59</p></td>
<td><p>N/A</p></td>
</tr>
<tr class="row-even"><td><p>8s Gen 3 (SM8635)</p></td>
<td><p>aarch64-android</p></td>
<td><p>68</p></td>
<td><p>V73</p></td>
</tr>
<tr class="row-odd"><td><p>7+ Gen 3 (SM7675)</p></td>
<td><p>aarch64-android</p></td>
<td><p>70</p></td>
<td><p>V73</p></td>
</tr>
</tbody>
</table>
</section>
<section id="software-architecture">
<h2>Software architecture<a class="headerlink" href="#software-architecture" title="Link to this heading"></a></h2>
<p>The Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> API and the associated software stack provides all the constructs required by an application
to construct, optimize, and execute network models on the preferred hardware accelerator core.</p>
<p>Key constructs are shown in <a class="reference internal" href="#qnn-highlevel-view-figure"><span class="std std-ref">Qualcomm AI Engine Direct Components - High Level View</span></a>.</p>
<p class="centered" id="qnn-highlevel-view-figure">
<strong><strong>Qualcomm AI Engine Direct Components - High Level View</strong></strong></p><figure class="align-center">
<img alt="QNN high-level view" src="../_images/qnn_highlevel_view.png" />
</figure>
<section id="device">
<h3>Device<a class="headerlink" href="#device" title="Link to this heading"></a></h3>
<p>The software abstraction of a hardware accelerator platform. Provides all constructs required to associate the preferred hardware
accelerator resources for execution of user-composed graphs. A platform is broken down into potentially multiple
devices. Devices may have multiple cores.</p>
</section>
<section id="backend">
<h3>Backend<a class="headerlink" href="#backend" title="Link to this heading"></a></h3>
<p>The backend is a top-level API component that hosts and manages most of the backend resources required for graph
composition and execution, including an operation registry that stores all available operations.</p>
<p>Learn more about the Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> backends <span class="xref std std-doc">here</span>.</p>
</section>
<section id="context">
<h3>Context<a class="headerlink" href="#context" title="Link to this heading"></a></h3>
<p>A construct that represents all Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> components required to sustain a user application. Hosts networks provided by the
user and allows constructed entities to be cached into serialized objects for future use. It enables interoperability
between multiple graphs by providing a shareable memory space in which tensors can be exchanged between graphs.</p>
</section>
<section id="graph">
<h3>Graph<a class="headerlink" href="#graph" title="Link to this heading"></a></h3>
<p>The Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> way of representing a loadable network model. Consists of nodes that represent
operations and tensors that interconnect them to compose a directed acyclic graph.
The Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> graph construct supports APIs that perform initialization, optimization, and execution of
network models.</p>
</section>
<section id="operation-package-registry">
<h3>Operation Package registry<a class="headerlink" href="#operation-package-registry" title="Link to this heading"></a></h3>
<p>A registry that maintains a record of all operations available to execute a model.
These operations can be built-in or supplied by the user as custom operations.</p>
<p>Learn more about Operation Packages <span class="xref std std-doc">here</span>.</p>
</section>
</section>
<section id="integration-workflow">
<h2>Integration workflow<a class="headerlink" href="#integration-workflow" title="Link to this heading"></a></h2>
<p>The Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> SDK provides tools and extensible per-accelerator libraries with uniform API, enabling
flexible integration and efficient execution of ML/DL neural networks on QTI chipsets. The Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> API
is designed to support inference of trained neural networks and, as such, clients are responsible for
training a ML/DL network in a training framework of their choice. The training process is typically
performed on server hosts, off-device. Once a network is trained, clients can use Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> to get it ready to
deploy and run on-device.</p>
<p>This workflow is shown in
<a class="reference internal" href="#training-inference-workflow-figure"><span class="std std-ref">Training vs. Inference Workflow</span></a>.</p>
<p class="centered" id="training-inference-workflow-figure">
<strong><strong>Training vs. Inference Workflow</strong></strong></p><figure class="align-center">
<img alt="Training vs. Inference Workflow" src="../_images/training_inference_workflow.png" />
</figure>
<p>The Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> SDK includes tools to aid clients in integrating trained DL networks into their
applications.</p>
<p>The basic integration workflow is shown in
<a class="reference internal" href="#qnn-basic-workflow-figure"><span class="std std-ref">Qualcomm AI Engine Direct Integration Workflow</span></a>.</p>
<p class="centered" id="qnn-basic-workflow-figure">
<strong><strong>Qualcomm AI Engine Direct Integration Workflow</strong></strong></p><figure class="align-center">
<img alt="Basic QNN Integration Workflow" src="../_images/qnn_basic_workflow.png" />
</figure>
<ol class="arabic">
<li><p>Clients call the Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> converter tool by providing their trained network model file as input.
The network must be trained in a framework supported by the Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> converter tools.
See <span class="xref std std-doc">Tools</span> for more details on Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> converters.</p></li>
<li><p>When source models contain operations that are not supported natively by Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> backends,
clients must provide OpPackage definition files to the converter, expressing custom/client-defined
operations. Optionally, users can use the OpPackage generator tool to generate
skeleton code to implement and compile custom operations into OpPackage libraries.
See <span class="xref std std-ref">general/tools:qnn-op-package-generator</span> for usage details.</p></li>
<li><p>The Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> model converter is a tool to aid clients in writing a sequence of Qualcomm® <span class="xref std std-ref">AI Engine Direct</span>
API calls to construct a Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> graph representation of a trained network that was provided as input to the tool.
The converter outputs the following files:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">.cpp</span></code> – Source file (e.g., <code class="docutils literal notranslate"><span class="pre">model.cpp</span></code>) containing required Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> API calls to construct a
network graph</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">.bin</span></code> – Binary file (e.g., <code class="docutils literal notranslate"><span class="pre">model.bin</span></code>) containing network weights and biases as float32 data.</p></li>
</ul>
<p>Clients can optionally direct the converter to output a quantized model instead of the default one,
as indicated in the diagram above as <em>quantized model.cpp</em>. In this case, the <code class="docutils literal notranslate"><span class="pre">model.bin</span></code> file will
contain quantized data, and <code class="docutils literal notranslate"><span class="pre">model.cpp</span></code> will reference quantized tensor data types and
include quantization encodings. Quantized models may be required by some Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> backend libraries,
e.g., HTP or DSP (see <span class="xref std std-ref">general/api:Backend Supplements</span> for information on supported
data types). For details on converter quantization function and options, see
<span class="xref std std-ref">general/tools:Quantization Support</span>.</p>
</li>
<li><p>Clients optionally can use the Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> model library generator tool to produce a model library.
See <span class="xref std std-ref">general/tools:qnn-model-lib-generator</span> for usage details.</p></li>
<li><p>Clients integrate the Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> model into their application by either dynamically loading a model library
or compiling and statically linking <code class="docutils literal notranslate"><span class="pre">model.cpp</span></code> and <code class="docutils literal notranslate"><span class="pre">model.bin</span></code> files.
To prepare and execute the model (i.e., run inference), clients must load the required
Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> backend accelerator and OpPackage libraries. The Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> OpPackage libraries are registered
with and loaded by the backend.</p></li>
<li><p>Clients can optionally save the context binary cache with prepared and finalized graphs. See
<span class="xref std std-ref">general/api_overview:Context Caching</span> for reference.
Such graphs can be repeatedly loaded from the cache without the need for <code class="docutils literal notranslate"><span class="pre">model.cpp</span></code>/ library.
Loading a model graph from the cache is significantly faster than preparing through
a sequence of graph composition API calls provided in <code class="docutils literal notranslate"><span class="pre">model.cpp</span></code>/ library. Cached graphs cannot
be further modified; they are meant for deployment of prepared graphs, enabling faster
initialization of client applications.</p></li>
</ol>
<ol class="arabic simple" start="7">
<li><p>Clients can optionally utilize Deep Learning Containers (DLCs) produced from <a class="reference external" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-2/introduction.html">Qualcomm Neural Processing SDK</a>
in conjunction with the provided <code class="docutils literal notranslate"><span class="pre">libQnnModelDlc.so</span></code> library to produce QNN graph handles from DLC paths in their application. This provides a single format
for use across products and support for large models that cannot be compiled into a shared model library. Details on usage
can be found in <span class="xref std std-doc">Utilizing DLCs</span>.</p></li>
</ol>
</section>
<section id="developers-on-linux">
<h2>Developers on Linux<a class="headerlink" href="#developers-on-linux" title="Link to this heading"></a></h2>
<p>Executables and libraries can be found under the target folder of the SDK with <em>linux</em>, <em>ubuntu</em>, or <em>android</em> in the name.
See <span class="xref std std-ref">Release Folder for Different Platforms</span> for reference.
The library naming convention should follow the rules of Linux. See <span class="xref std std-ref">Library and Executable Program Naming Convention</span> for reference.
The operation mentioned above can be run under Linux OS, such as Ubuntu system.</p>
</section>
<section id="integration-workflow-on-windows">
<h2>Integration workflow on Windows<a class="headerlink" href="#integration-workflow-on-windows" title="Link to this heading"></a></h2>
</section>
<section id="developers-on-windows">
<span id="workflow-wsl"></span><h2>Developers on Windows<a class="headerlink" href="#developers-on-windows" title="Link to this heading"></a></h2>
<p>The Qualcomm SDK provide three different platforms for Windows host. For users familiar to Linux operation,
we suggest using WSL (Windows Subsystem for Linux) on Windows. For developers who want to use tools on Windows-PC directly through the PowerShell environment,
Qualcomm® <span class="xref std std-ref">AI Engine Direct</span> provides tools based on x86_64-windows. Check the following prerequisites.</p>
<section id="wsl-platform">
<h3>WSL platform<a class="headerlink" href="#wsl-platform" title="Link to this heading"></a></h3>
<p>For WSL developers:
The workflow on a Windows host is the same as on a Linux host, though some steps will require execution on WSL (x86)
and others will be executed natively on Windows as outlined below. Because WSL is run on a GNU/Linux environment, model tools
and libraries should get from x86_64-linux-clang respectively.
To understand more about the WSL setup, visit <span class="xref std std-ref">Linux Platform Dependency</span>.</p>
</section>
<section id="windows-platform">
<span id="workflow-windows-native"></span><h3>Windows platform<a class="headerlink" href="#windows-platform" title="Link to this heading"></a></h3>
<p>For Windows native/x86_64 PC developers:
The tools and libraries are located within the <code class="docutils literal notranslate"><span class="pre">x86_64-windows-msvc</span></code> folder. Tools are highly related to Python version and setting. Therefore,
the environment setup for PowerShell is required before operation. Refer to the settings for the Windows
<span class="xref std std-ref">Windows Platform Dependencies</span>.</p>
<p>For Windows on Snapdragon developers:
The tools and libraries are located within the <code class="docutils literal notranslate"><span class="pre">aarch64-windows-msvc</span></code> folder. The environment setup for PowerShell is required before operation.
Refer to the settings for the Windows
<span class="xref std std-ref">Windows Platform Dependencies</span>.</p>
<ol class="arabic simple">
<li><p>For OP Customization, the Op Package skeleton code is generated by running the Linux OpPackage Generator tool on WSL (x86).</p></li>
<li><p>For Context Binary Generation, clients can use either the Linux Context Binary Generator tool on WSL (x86)
or windows-native powershell. The tool executable and libraries used must be from the corresponding folder, as mentioned in <span class="xref std std-ref">Release Folder for Different Platforms</span>.</p></li>
<li><p>For Model Library Generation, the model library is produced by running the Windows Model Library Generator tool natively on Windows.</p></li>
<li><p>Tools mentioned in the integration workflow can be applied through WSL or Windows native x86.
The supporting list of the tools by platform is demonstrated in <span class="xref std std-doc">Tools</span>.</p></li>
<li><p>The ARM64X package format is supported for CPU and HTP backend on SC8380XP. The tools and libraries are located within the <code class="docutils literal notranslate"><span class="pre">arm64x-windows-msvc</span></code> folder.
See <span class="xref std std-doc">ARM64X Tutorial</span> for the usage and details.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using WSL, the Model Tools must be obtained from the <code class="docutils literal notranslate"><span class="pre">linux</span></code> folder as it is a subsystem for Linux.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When run on Windows natively, the Model Library Generation Tool must be run with python.
See <a class="reference internal" href="tutorial2.html#windows-model-lib-generator"><span class="std std-ref">Model Build on Windows Host</span></a> section for an example.</p>
</div>
<p><strong>Notes</strong></p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id3" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Future feature.</p>
</aside>
<aside class="footnote brackets" id="id4" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>The SOC Model number is intended to be used in QNN API calls, for example when <span class="xref std std-ref">configuring the QNN HTP backend</span>.</p>
</aside>
</aside>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="introduction.html" class="btn btn-neutral float-left" title="Introduction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tutorials.html" class="btn btn-neutral float-right" title="Tutorials" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2024, Qualcomm Technologies, Inc..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>