<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Windows Target Device (10, 11, and Snapdragon) &mdash; Qualcomm® AI Engine Direct</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/custom_css.css?v=22c3e01d" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=474e5199"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Setup" href="setup.html" />
    <link rel="prev" title="CNN to QNN for Windows Host" href="qnn_tutorial_windows_1.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Qualcomm® AI Engine Direct
          </a>
              <div class="version">
                2.24
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="tutorials.html">Tutorials</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="tutorials.html#getting-started">Getting Started</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="qnn_tutorial_linux_1.html">CNN to QNN for Linux Host</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="qnn_tutorial_windows_1.html">CNN to QNN for Windows Host</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="qnn_tutorial_windows_1.html#tutorial-setup">Tutorial Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="qnn_tutorial_windows_1.html#converting-the-cnn-model-into-a-qnn-model">Converting the CNN model into a QNN model</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="qnn_tutorial_windows_1.html#build-the-model-for-a-target-device">Build the Model for a Target Device</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="tutorials.html#advanced">Advanced</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorials.html#custom-operators">Custom Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorials.html#windows">Windows</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorials.html#migrating">Migrating</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Setup</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Qualcomm® AI Engine Direct</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="tutorials.html">Tutorials</a></li>
          <li class="breadcrumb-item"><a href="qnn_tutorial_windows_1.html">CNN to QNN for Windows Host</a></li>
      <li class="breadcrumb-item active">Windows Target Device (10, 11, and Snapdragon)</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="windows-target-device-10-11-and-snapdragon">
<h1>Windows Target Device (10, 11, and Snapdragon)<a class="headerlink" href="#windows-target-device-10-11-and-snapdragon" title="Link to this heading"></a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This section contains the final two steps of the CNN to QNN tutorial. If you have not completed steps 1 and 2, please return to the starting page <a class="reference internal" href="qnn_tutorial_windows_1.html"><span class="doc">here</span></a>.</p>
</div>
<section id="build-your-qnn-model-for-target-device-architecture">
<h2>Build your QNN model for target device architecture<a class="headerlink" href="#build-your-qnn-model-for-target-device-architecture" title="Link to this heading"></a></h2>
<p>Once the CNN model has been converted into QNN format, the next step is to build it so it can run on the target device’s operating system with <code class="docutils literal notranslate"><span class="pre">qnn-model-lib-generator</span></code>.</p>
<p>Based on the operating system and architecture of your target device, choose one of the following build instructions.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>For cases where the “host machine” and “target device” are the same (e.g., you want to build and run model inferences on your Snapdragon for Windows device), you will need to adapt the steps to copy files to your current device instead of a remote device.</p>
</div>
<ol class="arabic">
<li><p>Ensure you have <code class="docutils literal notranslate"><span class="pre">cmake</span></code> installed on your machine by running <code class="docutils literal notranslate"><span class="pre">cmake</span> <span class="pre">--version</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="docutils literal notranslate"><span class="pre">cmake</span></code> is not installed, run <code class="docutils literal notranslate"><span class="pre">&amp;</span> <span class="pre">&quot;${QNN_SDK_ROOT}/bin/check-windows-dependency.ps1&quot;</span></code> to download the proper dependencies.</p>
</div>
</li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">mkdir</span> <span class="pre">C:\tmp\qnn_tmp</span></code> to make the folder where your newly built files will live.</p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">C:\tmp\qnn_tmp</span></code> to navigate to the new folder.</p></li>
<li><p>Run the following command to copy over the QNN model files:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>Copy-Item<span class="w"> </span>-Path<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">env</span><span class="p">:</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">\examples\Models\InceptionV3\model\Inception_v3.cpp&quot;</span>,<span class="s2">&quot;</span><span class="si">${</span><span class="nv">env</span><span class="p">:</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">\examples\Models\InceptionV3\model\Inception_v3.bin&quot;</span><span class="w"> </span>-Destination<span class="w"> </span><span class="s2">&quot;c:\tmp\qnn_tmp&quot;</span>
</pre></div>
</div>
</li>
<li><p>Determine whether your host machine’s operating system is <code class="docutils literal notranslate"><span class="pre">windows-x86_64</span></code> or using ARM architecture <code class="docutils literal notranslate"><span class="pre">windows-aarch64</span></code>. (For Snapdragon devices, choose <code class="docutils literal notranslate"><span class="pre">windows-aarch64</span></code>)</p></li>
<li><p>Run the following command, updating the <code class="docutils literal notranslate"><span class="pre">-t</span></code> value with the target architecture.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">\bin\x86_64-windows-msvc\qnn-model-lib-generator&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-c<span class="w"> </span><span class="s2">&quot;.\Inception_v3.cpp&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-b<span class="w"> </span><span class="s2">&quot;.\Inception_v3.bin&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-o<span class="w"> </span><span class="s2">&quot;model_libs&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-t<span class="w"> </span><span class="s2">&quot;windows-x86_64&quot;</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-c</span></code> - This indicates the path to the <code class="docutils literal notranslate"><span class="pre">.cpp</span></code> QNN model file.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-b</span></code> - This indicates the path to the <code class="docutils literal notranslate"><span class="pre">.bin</span></code> QNN model file. (<code class="docutils literal notranslate"><span class="pre">-b</span></code> is optional, but at runtime the <code class="docutils literal notranslate"><span class="pre">.cpp</span></code> file could fail if it needs the <code class="docutils literal notranslate"><span class="pre">.bin</span></code> file, so it is recommended).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-o</span></code> - The path to the output folder.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-t</span></code> - Indicate which architecture to build for (between <code class="docutils literal notranslate"><span class="pre">windows-x86_64</span></code> and <code class="docutils literal notranslate"><span class="pre">windows-aarch64</span></code>)</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If the build fails due to a missing build dependency such as cmake or clang-cl being missing, run <code class="docutils literal notranslate"><span class="pre">&amp;</span> <span class="pre">&quot;${QNN_SDK_ROOT}/bin/check-windows-dependency.ps1&quot;</span></code> to install all build dependencies.</p>
<p>You can also use <code class="docutils literal notranslate"><span class="pre">&amp;</span> <span class="pre">&quot;${QNN_SDK_ROOT}/bin/envcheck.ps1&quot;</span> <span class="pre">-a</span></code> to help debug which dependencies are missing.</p>
</div>
</li>
<li><p>The output <code class="docutils literal notranslate"><span class="pre">.dll</span></code> file will be inside <code class="docutils literal notranslate"><span class="pre">model_libs</span></code> with a name that corresponds to the target architecture specified with <code class="docutils literal notranslate"><span class="pre">-t</span></code> in the above command.
* Example: <code class="docutils literal notranslate"><span class="pre">model_libs/x64/Inception_v3.dll</span></code> or <code class="docutils literal notranslate"><span class="pre">model_libs/aarch64/Inception_v3.dll</span></code></p></li>
</ol>
<p>That <code class="docutils literal notranslate"><span class="pre">Inception_v3.dll</span></code> file is what you will use on the target device to actually execute inferences. See the instructions below for how to run inferences on the specific processors you care about using <code class="docutils literal notranslate"><span class="pre">Inception_v3.dll</span></code>.</p>
</section>
<section id="use-the-built-model-on-specific-processors">
<h2>Use the built model on specific processors<a class="headerlink" href="#use-the-built-model-on-specific-processors" title="Link to this heading"></a></h2>
<p>Now that you have an executable version of your model, the next step is to transfer it to the target processor and run inferences with it. If you haven’t already, ensure that you follow the processor-specific Setup instructions here.</p>
<p>Pick which processor below you would like to run your model on for specific instructions on transferring the relevant files, then executing an inference with your model.</p>
</section>
<section id="shared-steps">
<h2>Shared Steps<a class="headerlink" href="#shared-steps" title="Link to this heading"></a></h2>
<ol class="arabic">
<li><p>Run <code class="docutils literal notranslate"><span class="pre">&amp;</span> <span class="pre">ls</span> <span class="pre">${QNN_SDK_ROOT}/bin</span></code> to see the list of all supported target device architectures. (Each folder represents a different architecture)</p></li>
<li><p>Set the terminal variable <code class="docutils literal notranslate"><span class="pre">$QNN_TARGET_ARCH</span></code> to the name of your target device’s architecture.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">$QNN_TARGET_ARCH</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;your-target-architecture (ex. aarch64-windows-msvc)&quot;</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="cpu">
<h2>CPU<a class="headerlink" href="#cpu" title="Link to this heading"></a></h2>
<p><strong>Moving over all relevant files:</strong></p>
<ol class="arabic">
<li><p>Connect to the target device with <code class="docutils literal notranslate"><span class="pre">mstsc</span> <span class="pre">-v</span> <span class="pre">&lt;your</span> <span class="pre">device</span> <span class="pre">IP&gt;</span></code>.</p></li>
<li><p>On the target device, create the folder <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code>.</p></li>
<li><p>Copy the following files from your host machine to the target device’s <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code>:
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}\lib\${QNN_TARGET_ARCH}\QnnCpu.dll</span></code>
* <code class="docutils literal notranslate"><span class="pre">C:\tmp\qnn_tmp\model_lib\x64\Inception_v3.dll</span></code> - This is the built example model from above.</p>
<blockquote>
<div><ul class="simple">
<li><p>Note that the <code class="docutils literal notranslate"><span class="pre">x64</span></code> folder may be different depending on your host computer’s architecture. (<code class="docutils literal notranslate"><span class="pre">x64</span></code> is for <code class="docutils literal notranslate"><span class="pre">x86_64</span></code> architecture).</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Copy the input data and input list from the QNN SDK examples folder into <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code>:
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped</span></code>
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/target_raw_list.txt</span></code>
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/imagenet_slim_labels</span></code>
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/scripts/show_inceptionv3_classifications.py</span></code></p></li>
<li><p>Copy <code class="docutils literal notranslate"><span class="pre">qnn-net-run.exe</span></code> from <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}\bin\${QNN_TARGET_ARCH}\qnn-net-run.exe</span></code> to <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code>.</p></li>
</ol>
<p><strong>Doing inferences on the target device processor:</strong></p>
<ol class="arabic">
<li><p>Open a PowerShell instance on the target device.</p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">C:\qnn_test_package</span></code>.</p></li>
<li><p>Run the following command to create an inference:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>.<span class="se">\q</span>nn-net-run.exe<span class="w"> </span><span class="sb">`</span>
<span class="w">   </span>--model<span class="w"> </span>.<span class="se">\I</span>nception_v3.dll<span class="w"> </span><span class="sb">`</span>
<span class="w">   </span>--input_list<span class="w"> </span>.<span class="se">\t</span>arget_raw_list.txt<span class="w"> </span><span class="sb">`</span>
<span class="w">   </span>--backend<span class="w"> </span>.<span class="se">\Q</span>nnCpu.dll
<span class="w">   </span>--output<span class="w"> </span>.<span class="se">\o</span>utput
</pre></div>
</div>
</li>
<li><p>Run the following script to create a view of the classification results:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can alternatively copy the output folder back to your Windows host machine and run the following script there to avoid having to install python on your target device.</p>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>py<span class="w"> </span>-3<span class="w"> </span>.<span class="se">\s</span>how_inceptionv3_classifications.py<span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-i<span class="w"> </span>.<span class="se">\c</span>ropped<span class="se">\r</span>aw_list.txt<span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-o<span class="w"> </span>output<span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-l<span class="w"> </span>.<span class="se">\i</span>magenet_slim_labels.txt
</pre></div>
</div>
</li>
<li><p>Verify that the classification results in <code class="docutils literal notranslate"><span class="pre">output</span></code> match the following:
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/trash_bin.raw</span>&#160;&#160; <span class="pre">0.777344</span> <span class="pre">413</span> <span class="pre">ashcan</span></code>
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/chairs.raw</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">0.253906</span> <span class="pre">832</span> <span class="pre">studio</span> <span class="pre">couch</span></code>
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/plastic_cup.raw</span> <span class="pre">0.980469</span> <span class="pre">648</span> <span class="pre">measuring</span> <span class="pre">cup</span></code>
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/notice_sign.raw</span> <span class="pre">0.167969</span> <span class="pre">459</span> <span class="pre">brass</span></code></p></li>
</ol>
</section>
<section id="gpu">
<h2>GPU<a class="headerlink" href="#gpu" title="Link to this heading"></a></h2>
<ol class="arabic">
<li><p>Connect to the target device with <code class="docutils literal notranslate"><span class="pre">mstsc</span> <span class="pre">-v</span> <span class="pre">&lt;your</span> <span class="pre">device</span> <span class="pre">IP&gt;</span></code>.</p></li>
<li><p>On the target device, create the folder <code class="docutils literal notranslate"><span class="pre">C:/qnn_test_package</span></code>.</p></li>
<li><p>Copy the following files from your host machine to the target device’s <code class="docutils literal notranslate"><span class="pre">C:\qnn_test_package</span></code>:
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}\lib\${QNN_TARGET_ARCH}\QnnGpu.dll</span></code>
* <code class="docutils literal notranslate"><span class="pre">C:\tmp\qnn_tmp\model_lib\x64\Inception_v3.dll</span></code> - This is the built example model from above.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">x64</span></code> may be a different folder depending on your host machine’s architecture.</p>
</div>
</li>
<li><p>Copy the input data and input list from the QNN SDK examples folder into <code class="docutils literal notranslate"><span class="pre">C:/qnn_test_package</span></code>:
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped</span></code>
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/target_raw_list.txt</span></code>
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/imagenet_slim_labels</span></code>
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/scripts/show_inceptionv3_classifications.py</span></code></p></li>
<li><p>Copy <code class="docutils literal notranslate"><span class="pre">qnn-net-run.exe</span></code> from <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/bin/${QNN_TARGET_ARCH}/qnn-net-run.exe</span></code> to <code class="docutils literal notranslate"><span class="pre">C:/qnn_test_package</span></code>.</p></li>
<li><p>Open a PowerShell instance on the target device.</p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">C:/qnn_test_package</span></code>.</p></li>
<li><p>Run the following command to create an inference:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./qnn-net-run.exe<span class="w"> </span><span class="sb">`</span>
<span class="w">   </span>--model<span class="w"> </span>./Inception_v3.dll<span class="w"> </span><span class="sb">`</span>
<span class="w">   </span>--input_list<span class="w"> </span>./target_raw_list.txt<span class="w"> </span><span class="sb">`</span>
<span class="w">   </span>--backend<span class="w"> </span>./QnnGpu.dll<span class="w"> </span><span class="sb">`</span>
<span class="w">   </span>--output<span class="w"> </span>./output
</pre></div>
</div>
</li>
<li><p>Run the following script to create a view of the classification results:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>py<span class="w"> </span>-3<span class="w"> </span>./show_inceptionv3_classifications.py<span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-i<span class="w"> </span>./cropped/raw_list.txt<span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-o<span class="w"> </span>./output<span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-l<span class="w"> </span>./imagenet_slim_labels.txt
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>You may have to install python on your target device to do this. Alternatively, you can also copy the <code class="docutils literal notranslate"><span class="pre">./output</span></code> folder onto your host machine and run <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/scripts/show_inceptionv3_classifications.py</span></code> instead.</p>
</div>
</li>
<li><p>Verify that the classification results in <code class="docutils literal notranslate"><span class="pre">output</span></code> match the following:
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/trash_bin.raw</span> <span class="pre">0.777344</span> <span class="pre">413</span> <span class="pre">ashcan</span></code>
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/chairs.raw</span> <span class="pre">0.253906</span> <span class="pre">832</span> <span class="pre">studio</span> <span class="pre">couch</span></code>
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/plastic_cup.raw</span> <span class="pre">0.980469</span> <span class="pre">648</span> <span class="pre">measuring</span> <span class="pre">cup</span></code>
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/notice_sign.raw</span> <span class="pre">0.167969</span> <span class="pre">459</span> <span class="pre">brass</span></code></p></li>
</ol>
</section>
<section id="dsp">
<h2>DSP<a class="headerlink" href="#dsp" title="Link to this heading"></a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DSP processors require quantized models instead of floating point models. If you do not have a quantized model, please follow Step 1 of the CNN to QNN tutorial to build one.</p>
</div>
<ol class="arabic">
<li><p>Connect to the Windows device with <code class="docutils literal notranslate"><span class="pre">mstsc</span> <span class="pre">-v</span> <span class="pre">&lt;your</span> <span class="pre">device</span> <span class="pre">IP&gt;</span></code>.</p></li>
<li><p>Create the folder <code class="docutils literal notranslate"><span class="pre">C:/qnn_test_package</span></code> on the device.</p></li>
<li><p>Look up your target device’s Snapdragon architecture in this <a class="reference external" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/overview.html#supported-snapdragon-devices">Supported Snapdragon Devices</a> table and set <code class="docutils literal notranslate"><span class="pre">$DSP_ARCH</span></code> to <code class="docutils literal notranslate"><span class="pre">hexagon-vXX</span></code> where XX is the version of your Hexagon Architecture. For example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">$DSP_ARCH</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;hexagon-v68&quot;</span>
</pre></div>
</div>
</li>
<li><p>Copy the necessary libraries from the Windows host to the target device <code class="docutils literal notranslate"><span class="pre">C:/qnn_test_package</span></code>:
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/lib/${QNN_TARGET_ARCH}/QnnDsp.dll</span></code>
* <code class="docutils literal notranslate"><span class="pre">C:/tmp/qnn_tmp/model_lib/ARM64/Inception_v3.dll</span></code> (generated above)</p></li>
<li><p>Copy the specific versions of your <code class="docutils literal notranslate"><span class="pre">$DSP_ARCH</span></code> files by replacing <code class="docutils literal notranslate"><span class="pre">libQnnDspV66Stub.so</span></code> and <code class="docutils literal notranslate"><span class="pre">libQnnDspV66Stub.so</span></code> with your version (e.g., <code class="docutils literal notranslate"><span class="pre">libQnnDspV69Stub.so</span></code> for v69):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="p">&amp;</span><span class="w"> </span>adb<span class="w"> </span>push<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">DSP_ARCH</span><span class="si">}</span><span class="s2">/unsigned/libQnnDspV66Skel.so&quot;</span><span class="w"> </span><span class="s2">&quot;/data/local/tmp/inception_v3&quot;</span>
<span class="p">&amp;</span><span class="w"> </span>adb<span class="w"> </span>push<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/libQnnDspV66Stub.so&quot;</span><span class="w"> </span><span class="s2">&quot;/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Copy the input data and input list from the Windows host to the Windows device <code class="docutils literal notranslate"><span class="pre">C:/qnn_test_package</span></code>:
* <code class="docutils literal notranslate"><span class="pre">C:/tmp/qnn_tmp/cropped</span></code>
* <code class="docutils literal notranslate"><span class="pre">C:/tmp/qnn_tmp/target_raw_list.txt</span></code></p></li>
<li><p>Copy the <code class="docutils literal notranslate"><span class="pre">qnn-net-run.exe</span></code> tool to the Windows device:
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/bin/${QNN_TARGET_ARCH}/qnn-net-run.exe</span></code></p></li>
</ol>
<p><strong>On the target device:</strong></p>
<ol class="arabic">
<li><p>Open “Administrator: Windows PowerShell”.</p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">C:/qnn_test_package</span></code>.</p></li>
<li><p>Run the following command to create an inference:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./qnn-net-run.exe<span class="w"> </span><span class="sb">`</span>
<span class="w">   </span>--model<span class="w"> </span>./Inception_v3_quantized.dll<span class="w"> </span><span class="sb">`</span>
<span class="w">   </span>--input_list<span class="w"> </span>./target_raw_list.txt<span class="w"> </span><span class="sb">`</span>
<span class="w">   </span>--backend<span class="w"> </span>./QnnDsp.dll
</pre></div>
</div>
</li>
<li><p>After the inference, check the classification result. By default, outputs from the run will be located in the <code class="docutils literal notranslate"><span class="pre">./output</span></code> directory. Copy the results back to the Windows host.</p></li>
<li><p>Copy <code class="docutils literal notranslate"><span class="pre">C:/qnn_test_package/output</span></code> to <code class="docutils literal notranslate"><span class="pre">C:/tmp/qnn_tmp</span></code>.</p></li>
<li><p>Open “Developer PowerShell for VS 2022”.</p></li>
<li><p>Run the following command to view the results:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>C:/tmp/qnn_tmp
py<span class="w"> </span>-3<span class="w"> </span>./show_inceptionv3_classifications.py<span class="w"> </span><span class="sb">`</span>
<span class="w">   </span>-i<span class="w"> </span>./cropped/raw_list.txt<span class="w"> </span><span class="sb">`</span>
<span class="w">   </span>-o<span class="w"> </span>output<span class="w"> </span><span class="sb">`</span>
<span class="w">   </span>-l<span class="w"> </span>./imagenet_slim_labels.txt
</pre></div>
</div>
</li>
<li><p>Verify that the classification results match the following:
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/trash_bin.raw</span> <span class="pre">0.777344</span> <span class="pre">413</span> <span class="pre">ashcan</span></code>
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/chairs.raw</span> <span class="pre">0.253906</span> <span class="pre">832</span> <span class="pre">studio</span> <span class="pre">couch</span></code>
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/plastic_cup.raw</span> <span class="pre">0.980469</span> <span class="pre">648</span> <span class="pre">measuring</span> <span class="pre">cup</span></code>
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/notice_sign.raw</span> <span class="pre">0.167969</span> <span class="pre">459</span> <span class="pre">brass</span></code></p></li>
</ol>
</section>
<section id="htp">
<h2>HTP<a class="headerlink" href="#htp" title="Link to this heading"></a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>HTP processors require quantized models instead of floating point models. If you do not have a quantized model, please follow Step 1 of the CNN to QNN tutorial to build one.</p>
</div>
<p><strong>Additional HTP Required Setup</strong></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Running the model on a target device’s HTP requires the generation of a serialized context.</p>
</div>
<ol class="arabic">
<li><p>Users can set the custom options and different performance modes to HTP Backend through the backend config. Please refer to <a class="reference external" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/htp_backend.html">QNN HTP Backend Extensions</a> for various options available in the config.</p></li>
<li><p>Refer to the example below for creating a backend config file for the QCS6490/QCM6490 target with mandatory options passed in:
* Update the following information based on your device’s <code class="docutils literal notranslate"><span class="pre">htp_arch</span></code>.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;graphs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;graph_names&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">                </span><span class="s2">&quot;Inception_v3&quot;</span>
<span class="w">            </span><span class="p">],</span>
<span class="w">            </span><span class="nt">&quot;vtcm_mb&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;devices&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;htp_arch&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;v68&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>The above config file with minimum parameters such as backend extensions config specified through JSON is given below:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;backend_extensions&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;shared_library_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;path_to_shared_library&quot;</span><span class="p">,</span><span class="w">  </span><span class="c1">// give path to shared extensions library (.dll)</span>
<span class="w">        </span><span class="nt">&quot;config_file_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;path_to_config_file&quot;</span><span class="w">         </span><span class="c1">// give path to backend config</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>To generate the context, update <code class="docutils literal notranslate"><span class="pre">&lt;path</span> <span class="pre">to</span> <span class="pre">JSON</span> <span class="pre">of</span> <span class="pre">backend</span> <span class="pre">extensions&gt;</span></code> below with the config you wrote above, then run the command in Windows PowerShell:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="p">&amp;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/bin/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/qnn-context-binary-generator.exe&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>--backend<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/QnnHtp.dll&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>--model<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/model_libs/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/libInception_v3.dll&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>--binary_file<span class="w"> </span><span class="s2">&quot;Inception_v3.serialized&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>--config_file<span class="w"> </span>&lt;path<span class="w"> </span>to<span class="w"> </span>JSON<span class="w"> </span>of<span class="w"> </span>backend<span class="w"> </span>extensions&gt;
</pre></div>
</div>
</li>
<li><p>This creates the serialized context at:
* <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/output/Inception_v3.serialized.bin</span></code></p></li>
</ol>
<p><strong>Running the built model</strong></p>
<ol class="arabic">
<li><p>Connect to the Windows target device and create a folder for the model files and input data (target specific):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mstsc<span class="w"> </span>-v<span class="w"> </span>&lt;your<span class="w"> </span>device<span class="w"> </span>IP&gt;
New-Item<span class="w"> </span>-Path<span class="w"> </span><span class="s2">&quot;C:/qnn_test_package&quot;</span><span class="w"> </span>-ItemType<span class="w"> </span>Directory
</pre></div>
</div>
</li>
<li><p>Look up your target device’s Snapdragon architecture in this <a class="reference external" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/overview.html#supported-snapdragon-devices">Supported Snapdragon Devices</a> table and set <code class="docutils literal notranslate"><span class="pre">$HTP_ARCH</span></code> to <code class="docutils literal notranslate"><span class="pre">hexagon-vXX</span></code> where XX is the version of your Hexagon Architecture. For example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">$HTP_ARCH</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;hexagon-v68&quot;</span>
</pre></div>
</div>
</li>
<li><p>Copy <code class="docutils literal notranslate"><span class="pre">QnnHtp.dll</span></code> and your built model (<code class="docutils literal notranslate"><span class="pre">Inception_v3.serialized.bin</span></code>) to your target device:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>Copy-Item<span class="w"> </span>-Path<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">HTP_ARCH</span><span class="si">}</span><span class="s2">/unsigned/*&quot;</span><span class="w"> </span>-Destination<span class="w"> </span><span class="s2">&quot;C:/qnn_test_package&quot;</span>
Copy-Item<span class="w"> </span>-Path<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/QnnHtp.dll&quot;</span><span class="w"> </span>-Destination<span class="w"> </span><span class="s2">&quot;C:/qnn_test_package&quot;</span>
Copy-Item<span class="w"> </span>-Path<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/output/Inception_v3.serialized.bin&quot;</span><span class="w"> </span>-Destination<span class="w"> </span><span class="s2">&quot;C:/qnn_test_package&quot;</span>
</pre></div>
</div>
</li>
<li><p>Copy the specific version of your $HTP_ARCH file by replacing <code class="docutils literal notranslate"><span class="pre">QnnHtpV68Stub.dll</span></code> with your version (e.g., <code class="docutils literal notranslate"><span class="pre">QnnHtpV69Stub.dll</span></code> for v69):</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>Copy-Item<span class="w"> </span>-Path<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/QnnHtpV68Stub.dll&quot;</span><span class="w"> </span>-Destination<span class="w"> </span><span class="s2">&quot;C:/qnn_test_package&quot;</span>
</pre></div>
</div>
</li>
<li><p>Copy the input data and input lists to your target device:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>Copy-Item<span class="w"> </span>-Path<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/data/cropped&quot;</span><span class="w"> </span>-Destination<span class="w"> </span><span class="s2">&quot;C:/qnn_test_package&quot;</span>
Copy-Item<span class="w"> </span>-Path<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/data/target_raw_list.txt&quot;</span><span class="w"> </span>-Destination<span class="w"> </span><span class="s2">&quot;C:/qnn_test_package&quot;</span>
</pre></div>
</div>
</li>
<li><p>Copy the <code class="docutils literal notranslate"><span class="pre">qnn-net-run.exe</span></code> tool which will actually execute the inferences:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>Copy-Item<span class="w"> </span>-Path<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/bin/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/qnn-net-run.exe&quot;</span><span class="w"> </span>-Destination<span class="w"> </span><span class="s2">&quot;C:/qnn_test_package&quot;</span>
</pre></div>
</div>
</li>
<li><p>Set up the environment on your target device by running:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">$env</span>:LD_LIBRARY_PATH<span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;C:/qnn_test_package&quot;</span>
<span class="nv">$env</span>:ADSP_LIBRARY_PATH<span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;C:/qnn_test_package&quot;</span>
</pre></div>
</div>
</li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> in the target device shell to execute the inference on the example inputs:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./qnn-net-run.exe<span class="w"> </span>--backend<span class="w"> </span>QnnHtp.dll<span class="w"> </span>--input_list<span class="w"> </span>target_raw_list.txt<span class="w"> </span>--retrieve_context<span class="w"> </span>Inception_v3.serialized.bin<span class="w"> </span>--output<span class="w"> </span>./output
</pre></div>
</div>
</li>
<li><p>Copy the results back to the Windows host machine:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>Copy-Item<span class="w"> </span>-Path<span class="w"> </span><span class="s2">&quot;C:/qnn_test_package/output&quot;</span><span class="w"> </span>-Destination<span class="w"> </span><span class="s2">&quot;C:/tmp/qnn_tmp&quot;</span>
</pre></div>
</div>
</li>
<li><p>Open “Developer PowerShell for VS 2022”</p></li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">C:/tmp/qnn_tmp</span></code></p></li>
<li><p>Run the following command to output a readable view of the inference data:</p></li>
</ol>
<blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>py<span class="w"> </span>-3<span class="w"> </span>./show_inceptionv3_classifications.py<span class="w"> </span>-i<span class="w"> </span>./cropped/raw_list.txt<span class="w"> </span>-o<span class="w"> </span>output<span class="w"> </span>-l<span class="w"> </span>./imagenet_slim_labels.txt
</pre></div>
</div>
</div></blockquote>
<ol class="arabic simple" start="13">
<li><p>Verify that the classification results in <code class="docutils literal notranslate"><span class="pre">output</span></code> match the following:</p></li>
</ol>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/trash_bin.raw</span>&#160;&#160; <span class="pre">0.777344</span> <span class="pre">413</span> <span class="pre">ashcan</span></code>
<code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/chairs.raw</span>&#160;&#160;&#160;&#160;&#160; <span class="pre">0.253906</span> <span class="pre">832</span> <span class="pre">studio</span> <span class="pre">couch</span></code>
<code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/plastic_cup.raw</span> <span class="pre">0.980469</span> <span class="pre">648</span> <span class="pre">measuring</span> <span class="pre">cup</span></code>
<code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/notice_sign.raw</span> <span class="pre">0.167969</span> <span class="pre">459</span> <span class="pre">brass</span></code></p>
</div></blockquote>
</section>
<section id="lpai">
<h2>LPAI<a class="headerlink" href="#lpai" title="Link to this heading"></a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>LPAI processors are designed for <strong>offline model preparation only</strong>. This means you must build the model first, then transfer it.</p>
</div>
<p>The offline generated model must be executed via the <a class="reference external" href="https://qpm.qualcomm.com/#/main/tools/details/LPAI">LPAI SDK</a>. You will have to sign in to access the LPAI SDK, and it may be dependent on filling out specific paperwork for your account to access the SDK.</p>
<p>You can use the following example files with the LPAI SDK to help run an inference on that processor:</p>
<p><strong>Example of ``config.json`` file:</strong></p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;backend_extensions&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;shared_library_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;${QNN_SDK_ROOT}/lib/x86_64-linux-clang/libQnnLpaiNetRunExtentions.so&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;config_file_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./lpaiParams.conf&quot;</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Example of ``lpaiParams.conf`` file:</strong></p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;lpai_backend&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;target_env&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;x86&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;enable_hw_ver&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;v4&quot;</span>
<span class="w">   </span><span class="p">},</span>
<span class="w">   </span><span class="nt">&quot;lpai_graph&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;prepare&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;enable_layer_fusion&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;enable_batchnorm_fold&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;enable_channel_align&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;pad_split&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;exclude_io&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
<span class="w">      </span><span class="p">},</span>
<span class="w">      </span><span class="nt">&quot;graph_name_0&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;prepare&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;enable_batchnorm_fold&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">         </span><span class="p">}</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>To configure <code class="docutils literal notranslate"><span class="pre">lpaiParams.conf</span></code>, consider using the following optional settings:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>lpai_backend
<span class="w">   </span><span class="s2">&quot;target_env&quot;</span><span class="w">              </span><span class="s2">&quot;arm/adsp/x86/tensilica, default adsp&quot;</span>
<span class="w">   </span><span class="s2">&quot;enable_hw_ver&quot;</span><span class="w">           </span><span class="s2">&quot;v1/v2/v3/v4, default v4&quot;</span>
lpai_graph
<span class="w">   </span>prepare
<span class="w">      </span><span class="s2">&quot;enable_layer_fusion&quot;</span><span class="w">     </span><span class="s2">&quot;true/false,     default true&quot;</span>
<span class="w">      </span><span class="s2">&quot;enable_batchnorm_fold&quot;</span><span class="w">   </span><span class="s2">&quot;true/false,     default false&quot;</span>
<span class="w">      </span><span class="s2">&quot;enable_channel_align&quot;</span><span class="w">    </span><span class="s2">&quot;true/false,     default false&quot;</span>
<span class="w">      </span><span class="s2">&quot;pad_split&quot;</span><span class="w">               </span><span class="s2">&quot;true/false,     default false&quot;</span>
<span class="w">      </span><span class="s2">&quot;exclude_io&quot;</span><span class="w">              </span><span class="s2">&quot;true/false,     default false&quot;</span>
</pre></div>
</div>
<p>Using the above <code class="docutils literal notranslate"><span class="pre">config.json</span></code> and <code class="docutils literal notranslate"><span class="pre">lpaiParams.conf</span></code> you can use <code class="docutils literal notranslate"><span class="pre">qnn-context-binary-generator</span></code> to build the LPAI offline model.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/examples/QNN/converter/models<span class="w"> </span><span class="sb">`</span>
<span class="p">&amp;</span><span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:<span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/lib/x86_64-linux-clang<span class="w"> </span><span class="sb">`</span>
<span class="p">&amp;</span><span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/bin/x86_64-linux-clang/qnn-context-binary-generator<span class="w"> </span><span class="sb">`</span>
<span class="w">              </span>--backend<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/lib/x86_64-linux-clang/libQnnLpai.so<span class="w"> </span><span class="sb">`</span>
<span class="w">              </span>--model<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/examples/Models/InceptionV3/model_libs/x86_64-linux-clang/&lt;libQnnModel.so&gt;<span class="w"> </span><span class="sb">`</span>
<span class="w">              </span>--config_file<span class="w"> </span>&lt;config.json&gt;<span class="w"> </span><span class="sb">`</span>
<span class="w">              </span>--log_level<span class="w"> </span>verbose<span class="w"> </span><span class="sb">`</span>
<span class="w">              </span>--backend_binary<span class="w"> </span>&lt;output_graph.eai&gt;<span class="w"> </span><span class="sb">`</span>
<span class="w">              </span>--binary_file<span class="w"> </span>tmp
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="qnn_tutorial_windows_1.html" class="btn btn-neutral float-left" title="CNN to QNN for Windows Host" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="setup.html" class="btn btn-neutral float-right" title="Setup" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2024, Qualcomm Technologies, Inc..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>