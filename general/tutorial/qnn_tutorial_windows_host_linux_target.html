<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Linux Target Device (Android / Linux Embedded / Ubuntu) &mdash; Qualcomm® AI Engine Direct</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom_css.css?v=22c3e01d" />

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../_static/documentation_options.js?v=474e5199"></script>
        <script src="../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Setup" href="../setup.html" />
    <link rel="prev" title="CNN to QNN for Windows Host" href="qnn_tutorial_windows_host.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Qualcomm® AI Engine Direct
          </a>
              <div class="version">
                2.24
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Overview</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../tutorials.html">Tutorials</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../tutorials.html#getting-started">Getting Started</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="qnn_tutorial_landing.html">Tutorial: Converting and executing a CNN model with QNN</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="qnn_tutorial_linux_host.html">CNN to QNN for Linux Host</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="qnn_tutorial_windows_host.html">CNN to QNN for Windows Host</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials.html#advanced">Advanced</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials.html#custom-operators">Custom Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials.html#windows">Windows</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials.html#migrating">Migrating</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../setup.html">Setup</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Qualcomm® AI Engine Direct</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../tutorials.html">Tutorials</a></li>
          <li class="breadcrumb-item"><a href="qnn_tutorial_landing.html">Tutorial: Converting and executing a CNN model with QNN</a></li>
          <li class="breadcrumb-item"><a href="qnn_tutorial_windows_host.html">CNN to QNN for Windows Host</a></li>
      <li class="breadcrumb-item active">Linux Target Device (Android / Linux Embedded / Ubuntu)</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="linux-target-device-android-linux-embedded-ubuntu">
<h1>Linux Target Device (Android / Linux Embedded / Ubuntu)<a class="headerlink" href="#linux-target-device-android-linux-embedded-ubuntu" title="Link to this heading"></a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is <strong>Part 2</strong> of the CNN to QNN tutorial for Windows host machines. If you have not completed Part 1, please do so <a class="reference internal" href="qnn_tutorial_windows_host.html"><span class="doc">here</span></a>.</p>
</div>
<section id="step-3-model-build-on-windows-host-for-linux-target">
<h2>Step 3: Model Build on Windows Host for Linux Target<a class="headerlink" href="#step-3-model-build-on-windows-host-for-linux-target" title="Link to this heading"></a></h2>
<p>Once the CNN model has been converted into QNN format, the next step is to build it so it can run on the target device’s operating system with <code class="docutils literal notranslate"><span class="pre">qnn-model-lib-generator</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please continue to use the same terminal you were using on your host machine from part 1.</p>
</div>
<p>Based on the operating system and architecture of your target device, choose one of the following build instructions.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>For cases where the “host machine” and “target device” are the same, you will need to adapt the steps to handle files locally instead of transferring them to a remote device.</p>
</div>
<ol class="arabic">
<li><p>Create a directory on your host machine where your newly built files will live by running:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>/tmp/qnn_tmp
</pre></div>
</div>
</li>
<li><p>Navigate to the new directory:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/tmp/qnn_tmp
</pre></div>
</div>
</li>
<li><p>Copy over the QNN <code class="docutils literal notranslate"><span class="pre">.cpp</span></code> and <code class="docutils literal notranslate"><span class="pre">.bin</span></code> model files:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>Copy-Item<span class="w"> </span>-Path<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$env</span><span class="s2">:QNN_SDK_ROOT\examples\Models\InceptionV3\model\Inception_v3.cpp&quot;</span>,<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$env</span><span class="s2">:QNN_SDK_ROOT\examples\Models\InceptionV3\model\Inception_v3.bin&quot;</span><span class="w"> </span>-Destination<span class="w"> </span><span class="s2">&quot;C:\tmp\qnn_tmp&quot;</span>
</pre></div>
</div>
</li>
<li><p>Choose the most relevant supported target architecture from the following list:</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you don’t know which one to choose, you can run the following commands on your target device to get more information: <code class="docutils literal notranslate"><span class="pre">uname</span> <span class="pre">-a</span></code>, <code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">/etc/os-release</span></code>, and <code class="docutils literal notranslate"><span class="pre">gcc</span> <span class="pre">--version</span></code>.</p>
</div>
<ul class="simple">
<li><p>For 64-bit Linux targets: <code class="docutils literal notranslate"><span class="pre">x86_64-linux-clang</span></code></p></li>
<li><p>For 64-bit Android devices: <code class="docutils literal notranslate"><span class="pre">aarch64-android</span></code></p></li>
<li><p>For Qualcomm’s QNX OS: <code class="docutils literal notranslate"><span class="pre">aarch64-qnx</span></code> (Note: This architecture is not supported by default in the QNN SDK.)</p></li>
<li><p>For OpenEmbedded Linux (GCC 11.2): <code class="docutils literal notranslate"><span class="pre">aarch64-oe-linux-gcc11.2</span></code></p></li>
<li><p>For OpenEmbedded Linux (GCC 9.3): <code class="docutils literal notranslate"><span class="pre">aarch64-oe-linux-gcc9.3</span></code></p></li>
<li><p>For OpenEmbedded Linux (GCC 8.2): <code class="docutils literal notranslate"><span class="pre">aarch64-oe-linux-gcc8.2</span></code></p></li>
<li><p>For Ubuntu Linux (GCC 9.4): <code class="docutils literal notranslate"><span class="pre">aarch64-ubuntu-gcc9.4</span></code></p></li>
<li><p>For Ubuntu Linux (GCC 7.5): <code class="docutils literal notranslate"><span class="pre">aarch64-ubuntu-gcc7.5</span></code></p></li>
</ul>
</li>
<li><p>On your host machine, set the target architecture of your target device by setting <code class="docutils literal notranslate"><span class="pre">QNN_TARGET_ARCH</span></code> to your device’s target architecture:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">$QNN_TARGET_ARCH</span><span class="o">=</span><span class="s2">&quot;your-target-architecture-from-above&quot;</span>
</pre></div>
</div>
<p>For example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">$QNN_TARGET_ARCH</span><span class="o">=</span><span class="s2">&quot;x86_64-linux-clang&quot;</span>
</pre></div>
</div>
</li>
<li><p>Run the following command to generate the model library, updating the <code class="docutils literal notranslate"><span class="pre">t</span></code> value with the target architecture:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/bin/x86_64-linux-clang/qnn-model-lib-generator&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-c<span class="w"> </span><span class="s2">&quot;Inception_v3.cpp&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-b<span class="w"> </span><span class="s2">&quot;Inception_v3.bin&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-o<span class="w"> </span>model_libs<span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-t<span class="w"> </span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">c</span></code> - This indicates the path to the <code class="docutils literal notranslate"><span class="pre">.cpp</span></code> QNN model file.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">b</span></code> - This indicates the path to the <code class="docutils literal notranslate"><span class="pre">.bin</span></code> QNN model file. (<code class="docutils literal notranslate"><span class="pre">b</span></code> is optional, but at runtime, the <code class="docutils literal notranslate"><span class="pre">.cpp</span></code> file could fail if it needs the <code class="docutils literal notranslate"><span class="pre">.bin</span></code> file, so it is recommended).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">o</span></code> - The path to the output folder.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">t</span></code> - Indicate which architecture to build for.</p></li>
</ul>
</li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">ls</span> <span class="pre">/tmp/qnn_tmp/model_libs/${QNN_TARGET_ARCH}</span></code> and verify that the output file <code class="docutils literal notranslate"><span class="pre">libInception_v3.so</span></code> is inside.
- You will use the <code class="docutils literal notranslate"><span class="pre">libInception_v3.so</span></code> file on the target device to execute inferences.
- The output <code class="docutils literal notranslate"><span class="pre">.so</span></code> file will be located in the <code class="docutils literal notranslate"><span class="pre">model_libs</span></code> directory, named according to the target architecture.</p>
<blockquote>
<div><ul class="simple">
<li><p>For example: <code class="docutils literal notranslate"><span class="pre">model_libs/x64/Inception_v3.so</span></code> or <code class="docutils literal notranslate"><span class="pre">model_libs/aarch64/Inception_v3.so</span></code>.</p></li>
</ul>
</div></blockquote>
</li>
</ol>
</section>
<section id="step-4-use-the-built-model-on-specific-processors">
<h2>Step 4: Use the built model on specific processors<a class="headerlink" href="#step-4-use-the-built-model-on-specific-processors" title="Link to this heading"></a></h2>
<p>Now that you have an executable version of your model, the next step is to transfer the built model and all necessary files to the target processor, then to run inferences on it.</p>
<ol class="arabic simple">
<li><p>Install all necessary dependencies from Setup.</p></li>
<li><p>Follow the below SSH setup instructions.</p></li>
<li><p>Follow the instructions for each specific processor you want to run your model on.</p></li>
</ol>
<p><strong>Sub-Step 1: If you haven’t already, ensure that you follow the processor-specific Setup instructions for your host machine :doc:`here &lt;/general/setup&gt;`.</strong></p>
<p><strong>Sub-Step 2: Set up SSH on the target device.</strong></p>
<blockquote>
<div><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Here we use <code class="docutils literal notranslate"><span class="pre">OpenSSH</span></code> to copy files with <code class="docutils literal notranslate"><span class="pre">scp</span></code> later on and run scripts on the target device via <code class="docutils literal notranslate"><span class="pre">ssh</span></code>. If that does not work for your target device, feel free to use any other method of transferring the files over. (Ex. <code class="docutils literal notranslate"><span class="pre">adb</span></code> for android debugging or USB with manual terminal commands on the target device)</p>
</div>
<ol class="arabic simple">
<li><dl class="simple">
<dt>Ensure that both the host device and the target device are on the same network for this setup.</dt><dd><ul class="simple">
<li><p>Otherwise, <code class="docutils literal notranslate"><span class="pre">OpenSSH</span></code> requires port-forwarding to connect.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>On your target device, install <code class="docutils literal notranslate"><span class="pre">OpenSSH</span> <span class="pre">Server</span></code> if it is not already.</dt><dd><ul class="simple">
<li><p>Ex. For an Ubuntu device, this would look like:</p></li>
</ul>
</dd>
</dl>
</li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Update package lists</span>
sudo<span class="w"> </span>apt<span class="w"> </span>update

<span class="c1"># Install OpenSSH server</span>
sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>openssh-server

<span class="c1"># Check SSH service status</span>
sudo<span class="w"> </span>systemctl<span class="w"> </span>status<span class="w"> </span>ssh

<span class="c1"># Start SSH service if it&#39;s not running</span>
sudo<span class="w"> </span>systemctl<span class="w"> </span>start<span class="w"> </span>ssh

<span class="c1"># Enable SSH service to start on boot</span>
sudo<span class="w"> </span>systemctl<span class="w"> </span><span class="nb">enable</span><span class="w"> </span>ssh
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can turn off the OpenSSH Server service later by running <code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">systemctl</span> <span class="pre">stop</span> <span class="pre">ssh</span></code> if you want to.</p>
</div>
<ol class="arabic simple" start="3">
<li><p>On your target device, run <code class="docutils literal notranslate"><span class="pre">ifconfig</span></code> to get the IP address of your target device.</p></li>
<li><p>On your host machine, set a console variable for your target device’s <code class="docutils literal notranslate"><span class="pre">inet</span> <span class="pre">addr</span></code> address from above (replacing <code class="docutils literal notranslate"><span class="pre">127.0.0.1</span></code> below).</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">$TARGET_IP</span><span class="o">=</span><span class="s2">&quot;127.0.0.1&quot;</span>
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>Also set the username of the desired account on your target device (you can find it by running <code class="docutils literal notranslate"><span class="pre">whoami</span></code> on your target device if you are logged into the desired account).</p></li>
</ol>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">$TARGET_USER</span><span class="o">=</span><span class="s2">&quot;your-linux-account-username&quot;</span>
</pre></div>
</div>
<p>6. On your host machine, install <code class="docutils literal notranslate"><span class="pre">OpenSSH</span> <span class="pre">Client</span></code> by:
- Opening a Powershell as an administrator.
- Installing by running <code class="docutils literal notranslate"><span class="pre">Add-WindowsCapability</span> <span class="pre">-Online</span> <span class="pre">-Name</span> <span class="pre">OpenSSH.Client~~~~0.0.1.0</span></code>
- Verifying the installation by running <code class="docutils literal notranslate"><span class="pre">Get-WindowsCapability</span> <span class="pre">-Online</span> <span class="pre">|</span> <span class="pre">Where-Object</span> <span class="pre">Name</span> <span class="pre">-like</span> <span class="pre">'OpenSSH.Client*'</span></code></p>
<p>From this point on you should be able to <code class="docutils literal notranslate"><span class="pre">ssh</span></code> from powershell. You may need to open another PowerShell to do though.</p>
</div></blockquote>
<p><strong>Sub-Step 3: Follow the steps below for whichever processor you would like to run your model on.</strong></p>
<section id="cpu">
<h3>CPU<a class="headerlink" href="#cpu" title="Link to this heading"></a></h3>
<section id="transferring-over-all-relevant-files">
<h4>Transferring over all relevant files<a class="headerlink" href="#transferring-over-all-relevant-files" title="Link to this heading"></a></h4>
<ol class="arabic">
<li><p>On the target device, open a terminal and make a destination folder by running:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mount<span class="w"> </span>-o<span class="w"> </span>remount,rw<span class="w"> </span>/
mkdir<span class="w"> </span>-p<span class="w"> </span>/data/local/tmp
<span class="nb">cd</span><span class="w"> </span>/data/local/tmp
ln<span class="w"> </span>-s<span class="w"> </span>/etc/<span class="w"> </span>/data/local/tmp
chmod<span class="w"> </span>-R<span class="w"> </span><span class="m">777</span><span class="w"> </span>/data/local/tmp
mkdir<span class="w"> </span>-p<span class="w"> </span>/data/local/tmp/inception_v3
</pre></div>
</div>
</li>
<li><p>On the host device, use <code class="docutils literal notranslate"><span class="pre">scp</span></code> to transfer <code class="docutils literal notranslate"><span class="pre">libQnnCpu.so</span></code> from your host machine to <code class="docutils literal notranslate"><span class="pre">/data/local/tmp/inception_v3</span></code> on the target device.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/libQnnCpu.so&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">scp</span></code> to transfer the example built model.
- Update the <code class="docutils literal notranslate"><span class="pre">x64</span></code> folder below to the proper folder for your built model. The folder name depends on your host machine’s architecture.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span><span class="s2">&quot;/tmp/qnn_tmp/model_libs/x64/libInception_V3.so&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Transfer the input data, input list, and script from the QNN SDK examples folder into <code class="docutils literal notranslate"><span class="pre">/data/local/tmp/inception_v3</span></code> on the target device using <code class="docutils literal notranslate"><span class="pre">scp</span></code> in a similar way:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span>-r<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/data/cropped&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/data/target_raw_list.txt&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/data/imagenet_slim_labels&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/scripts/show_inceptionv3_classifications.py&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Transfer <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> from <code class="docutils literal notranslate"><span class="pre">$QNN_SDK_ROOT/bin/$QNN_TARGET_ARCH/qnn-net-run</span></code> to <code class="docutils literal notranslate"><span class="pre">/data/local/tmp/inception_v3</span></code> on the target device:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$QNN_SDK_ROOT</span><span class="s2">/bin/</span><span class="nv">$QNN_TARGET_ARCH</span><span class="s2">/qnn-net-run&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="doing-inferences-on-the-target-device-processor">
<h4>Doing inferences on the target device processor<a class="headerlink" href="#doing-inferences-on-the-target-device-processor" title="Link to this heading"></a></h4>
<ol class="arabic">
<li><p>Open a PowerShell instance on the target device.
- Alternatively, you can <code class="docutils literal notranslate"><span class="pre">ssh</span></code> from your host machine, run the following command to <code class="docutils literal notranslate"><span class="pre">ssh</span></code> into your target device.
- These console variables were set in the above instructions for “Transferring all relevant files”.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ssh<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You will have to login with your target device’s login for that username.</p>
</div>
</li>
<li><p>Navigate to the directory containing the test files:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/data/local/tmp/inception_v3
</pre></div>
</div>
</li>
<li><p>Run the following command on the target device to execute an inference:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./qnn-net-run<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="w"> </span><span class="s2">&quot;./libInception_V3.so&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--input_list<span class="w"> </span><span class="s2">&quot;./target_raw_list.txt&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--backend<span class="w"> </span><span class="s2">&quot;./libQnnCpu.so&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--output<span class="w"> </span><span class="s2">&quot;./output&quot;</span>
</pre></div>
</div>
</li>
<li><p>Run the following script on the target device to view the classification results:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can alternatively copy the output folder back to your host machine with <code class="docutils literal notranslate"><span class="pre">scp</span></code> and run the following script there to avoid having to install python on your target device.</p>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span><span class="s2">&quot;.\show_inceptionv3_classifications.py&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-i<span class="w"> </span><span class="s2">&quot;.\cropped\raw_list.txt&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-o<span class="w"> </span><span class="s2">&quot;output&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-l<span class="w"> </span><span class="s2">&quot;.\imagenet_slim_labels.txt&quot;</span>
</pre></div>
</div>
</li>
<li><p>Verify that the classification results in <code class="docutils literal notranslate"><span class="pre">output</span></code> match the following:
1. <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/trash_bin.raw</span> <span class="pre">0.777344</span> <span class="pre">413</span> <span class="pre">ashcan</span></code>
2. <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/chairs.raw</span> <span class="pre">0.253906</span> <span class="pre">832</span> <span class="pre">studio</span> <span class="pre">couch</span></code>
3. <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/plastic_cup.raw</span> <span class="pre">0.980469</span> <span class="pre">648</span> <span class="pre">measuring</span> <span class="pre">cup</span></code>
4. <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/notice_sign.raw</span> <span class="pre">0.167969</span> <span class="pre">459</span> <span class="pre">brass</span></code></p></li>
</ol>
</section>
</section>
<section id="gpu">
<h3>GPU<a class="headerlink" href="#gpu" title="Link to this heading"></a></h3>
<section id="id1">
<h4>Transferring over all relevant files<a class="headerlink" href="#id1" title="Link to this heading"></a></h4>
<ol class="arabic">
<li><p>On the target device, open a terminal and make a destination folder by running:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mount<span class="w"> </span>-o<span class="w"> </span>remount,rw<span class="w"> </span>/
mkdir<span class="w"> </span>-p<span class="w"> </span>/data/local/tmp
<span class="nb">cd</span><span class="w"> </span>/data/local/tmp
ln<span class="w"> </span>-s<span class="w"> </span>/etc/<span class="w"> </span>/data/local/tmp
chmod<span class="w"> </span>-R<span class="w"> </span><span class="m">777</span><span class="w"> </span>/data/local/tmp
mkdir<span class="w"> </span>-p<span class="w"> </span>/data/local/tmp/inception_v3
</pre></div>
</div>
</li>
<li><p>On the host device, use <code class="docutils literal notranslate"><span class="pre">scp</span></code> to transfer <code class="docutils literal notranslate"><span class="pre">libQnnGpu.so</span></code> from your host machine to <code class="docutils literal notranslate"><span class="pre">/data/local/tmp/inception_v3</span></code> on the target device.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/libQnnGpu.so&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">scp</span></code> to transfer the example built model.
- Update the <code class="docutils literal notranslate"><span class="pre">x64</span></code> folder below to the proper folder for your built model. The folder name depends on your host machine’s architecture.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span><span class="s2">&quot;/tmp/qnn_tmp/model_libs/x64/libInception_V3.so&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Transfer the input data, input list, and script from the QNN SDK examples folder into <code class="docutils literal notranslate"><span class="pre">/data/local/tmp/inception_v3</span></code> on the target device using <code class="docutils literal notranslate"><span class="pre">scp</span></code> in a similar way:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span>-r<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/data/cropped&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/data/target_raw_list.txt&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/data/imagenet_slim_labels&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/scripts/show_inceptionv3_classifications.py&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Transfer <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> from <code class="docutils literal notranslate"><span class="pre">$QNN_SDK_ROOT/bin/$QNN_TARGET_ARCH/qnn-net-run</span></code> to <code class="docutils literal notranslate"><span class="pre">/data/local/tmp/inception_v3</span></code> on the target device:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$QNN_SDK_ROOT</span><span class="s2">/bin/</span><span class="nv">$QNN_TARGET_ARCH</span><span class="s2">/qnn-net-run&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="id2">
<h4>Doing inferences on the target device processor<a class="headerlink" href="#id2" title="Link to this heading"></a></h4>
<ol class="arabic">
<li><p>Open a PowerShell instance on the target Windows device.
- Alternatively, you can <code class="docutils literal notranslate"><span class="pre">ssh</span></code> from your Linux host machine, run the following command to <code class="docutils literal notranslate"><span class="pre">ssh</span></code> into your target device.
- These console variables were set in the above instructions for “Transferring all relevant files”.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ssh<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You will have to login with your target device’s login for that username.</p>
</div>
</li>
<li><p>Navigate to the directory containing the test files:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/data/local/tmp/inception_v3
</pre></div>
</div>
</li>
<li><p>Run the following command on the target device to execute an inference:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./qnn-net-run<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="w"> </span><span class="s2">&quot;./libInception_V3.so&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--input_list<span class="w"> </span><span class="s2">&quot;./target_raw_list.txt&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--backend<span class="w"> </span><span class="s2">&quot;./libQnnGpu.so&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--output<span class="w"> </span><span class="s2">&quot;./output&quot;</span>
</pre></div>
</div>
</li>
<li><p>Run the following script on the target device to view the classification results:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can alternatively copy the output folder back to your host machine with <code class="docutils literal notranslate"><span class="pre">scp</span></code> and run the following script there to avoid having to install python on your target device.</p>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span><span class="s2">&quot;.\show_inceptionv3_classifications.py&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-i<span class="w"> </span><span class="s2">&quot;.\cropped\raw_list.txt&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-o<span class="w"> </span><span class="s2">&quot;output&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>-l<span class="w"> </span><span class="s2">&quot;.\imagenet_slim_labels.txt&quot;</span>
</pre></div>
</div>
</li>
<li><p>Verify that the classification results in <code class="docutils literal notranslate"><span class="pre">output</span></code> match the following:
1. <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/trash_bin.raw</span> <span class="pre">0.777344</span> <span class="pre">413</span> <span class="pre">ashcan</span></code>
2. <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/chairs.raw</span> <span class="pre">0.253906</span> <span class="pre">832</span> <span class="pre">studio</span> <span class="pre">couch</span></code>
3. <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/plastic_cup.raw</span> <span class="pre">0.980469</span> <span class="pre">648</span> <span class="pre">measuring</span> <span class="pre">cup</span></code>
4. <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/notice_sign.raw</span> <span class="pre">0.167969</span> <span class="pre">459</span> <span class="pre">brass</span></code></p></li>
</ol>
</section>
</section>
<section id="dsp">
<h3>DSP<a class="headerlink" href="#dsp" title="Link to this heading"></a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DSP processors require quantized models instead of full precision models. If you do not have a quantized model, please follow <a class="reference internal" href="qnn_tutorial_windows_host.html"><span class="doc">here</span></a> of the CNN to QNN tutorial to build one.</p>
</div>
<section id="id3">
<h4>Transferring over all relevant files<a class="headerlink" href="#id3" title="Link to this heading"></a></h4>
<ol class="arabic">
<li><p>On the target device, open a terminal and make a destination folder by running:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mount<span class="w"> </span>-o<span class="w"> </span>remount,rw<span class="w"> </span>/
mkdir<span class="w"> </span>-p<span class="w"> </span>/data/local/tmp
<span class="nb">cd</span><span class="w"> </span>/data/local/tmp
ln<span class="w"> </span>-s<span class="w"> </span>/etc/<span class="w"> </span>/data/local/tmp
chmod<span class="w"> </span>-R<span class="w"> </span><span class="m">777</span><span class="w"> </span>/data/local/tmp
mkdir<span class="w"> </span>-p<span class="w"> </span>/data/local/tmp/inception_v3
</pre></div>
</div>
</li>
<li><p>Determine your target device’s SnapDragon architecture by looking your chipset up in the <a class="reference external" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/overview.html#supported-snapdragon-devices">Supported Snapdragon Devices</a> table.</p></li>
<li><p>Update the “X” values below and run the commands to set <code class="docutils literal notranslate"><span class="pre">DSP_ARCH</span></code> to match the version number found in the above table.
1. Only the 2 digits at the end should update, and they should have the same version. Ex. For “V68”, the proper value would be <code class="docutils literal notranslate"><span class="pre">hexagon-v68</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">$DSP_VERSION</span><span class="o">=</span><span class="s2">&quot;XX&quot;</span>
<span class="nv">$DSP_ARCH</span><span class="o">=</span><span class="s2">&quot;hexagon-v</span><span class="si">${</span><span class="nv">DSP_VERSION</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
</li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">scp</span></code> to transfer <code class="docutils literal notranslate"><span class="pre">libQnnDsp.so</span></code> as well as other necessary executables from your host machine to <code class="docutils literal notranslate"><span class="pre">/data/local/tmp/inception_v3</span></code> on the target device.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$QNN_SDK_ROOT</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/libQnnDsp.so&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">DSP_ARCH</span><span class="si">}</span><span class="s2">/unsigned/libQnnDspV</span><span class="si">${</span><span class="nv">DSP_VERSION</span><span class="si">}</span><span class="s2">Skel.so&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/libQnnDspV</span><span class="si">${</span><span class="nv">DSP_VERSION</span><span class="si">}</span><span class="s2">Stub.so&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/model_libs/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/*&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Check the <a class="reference external" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/backend.html">Backend table</a> to see if there are any other processor-specific executables needed for your target processor (<code class="docutils literal notranslate"><span class="pre">DSP</span></code>) and your target device’s architecture (<code class="docutils literal notranslate"><span class="pre">$QNN_TARGET_ARCH</span></code>).
1. Use similar syntax above for <code class="docutils literal notranslate"><span class="pre">scp</span></code> to transfer any additional <code class="docutils literal notranslate"><span class="pre">.so</span></code> files listed <strong>below</strong> your selected target architecture in <a class="reference external" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/backend.html">this table</a>. <strong>(There may be none!)</strong></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Ensure you <code class="docutils literal notranslate"><span class="pre">scp</span></code> the <code class="docutils literal notranslate"><span class="pre">hexagon-v##</span></code> values (in addition to the other architecture files!)</p>
</div>
</li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">scp</span></code> to transfer the example built model.
1. Update the <code class="docutils literal notranslate"><span class="pre">x64</span></code> folder below to the proper folder for your built model. The folder name depends on your host machine’s architecture.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span><span class="s2">&quot;/tmp/qnn_tmp/model_libs/x64/libInception_V3.so&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Transfer the input data, input list, and script from the QNN SDK examples folder into <code class="docutils literal notranslate"><span class="pre">/data/local/tmp/inception_v3</span></code> on the target device using <code class="docutils literal notranslate"><span class="pre">scp</span></code> in a similar way:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span>-r<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/data/cropped&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/data/target_raw_list.txt&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/data/imagenet_slim_labels&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/scripts/show_inceptionv3_classifications.py&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Transfer <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> from <code class="docutils literal notranslate"><span class="pre">$QNN_SDK_ROOT/bin/$QNN_TARGET_ARCH/qnn-net-run</span></code> to <code class="docutils literal notranslate"><span class="pre">/data/local/tmp/inception_v3</span></code> on the target device:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/bin/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/qnn-net-run&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="id4">
<h4>Doing inferences on the target device processor<a class="headerlink" href="#id4" title="Link to this heading"></a></h4>
<ol class="arabic">
<li><p>Open a PowerShell instance on the target device.
1. Alternatively, you can <code class="docutils literal notranslate"><span class="pre">ssh</span></code> from your host machine, run the following command to <code class="docutils literal notranslate"><span class="pre">ssh</span></code> into your target device.
2. These console variables were set in the above instructions for “Transferring all relevant files”.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ssh<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You will have to log in with your target device’s login for that username.</p>
</div>
</li>
<li><p>Navigate to the directory containing the test files:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/data/local/tmp/inception_v3
</pre></div>
</div>
</li>
<li><p>Run the following command on the target device to execute an inference:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./qnn-net-run<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="w"> </span><span class="s2">&quot;./libInception_V3.so&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--input_list<span class="w"> </span><span class="s2">&quot;./target_raw_list.txt&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--backend<span class="w"> </span><span class="s2">&quot;./libQnnDsp.so&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--output<span class="w"> </span><span class="s2">&quot;./output&quot;</span>
</pre></div>
</div>
</li>
<li><p>Run the following script on the target device to view the classification results:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can alternatively copy the output folder back to your host machine with <code class="docutils literal notranslate"><span class="pre">scp</span></code> and run the following script there to avoid having to install python on your target device.</p>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span><span class="s2">&quot;.\show_inceptionv3_classifications.py&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-i<span class="w"> </span><span class="s2">&quot;.\cropped\raw_list.txt&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span><span class="s2">&quot;output&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-l<span class="w"> </span><span class="s2">&quot;.\imagenet_slim_labels.txt&quot;</span>
</pre></div>
</div>
</li>
<li><p>Verify that the classification results in <code class="docutils literal notranslate"><span class="pre">output</span></code> match the following:
1. <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/trash_bin.raw</span> <span class="pre">0.777344</span> <span class="pre">413</span> <span class="pre">ashcan</span></code>
2. <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/chairs.raw</span> <span class="pre">0.253906</span> <span class="pre">832</span> <span class="pre">studio</span> <span class="pre">couch</span></code>
3. <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/plastic_cup.raw</span> <span class="pre">0.980469</span> <span class="pre">648</span> <span class="pre">measuring</span> <span class="pre">cup</span></code>
4. <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/notice_sign.raw</span> <span class="pre">0.167969</span> <span class="pre">459</span> <span class="pre">brass</span></code></p></li>
</ol>
</section>
</section>
<section id="htp">
<h3>HTP<a class="headerlink" href="#htp" title="Link to this heading"></a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>HTP processors require quantized models instead of full precision models. If you do not have a quantized model, please follow <a class="reference internal" href="qnn_tutorial_windows_host.html"><span class="doc">here</span></a> of the CNN to QNN tutorial to build one.</p>
</div>
<section id="additional-htp-required-setup">
<h4>Additional HTP Required Setup<a class="headerlink" href="#additional-htp-required-setup" title="Link to this heading"></a></h4>
<p><em>Running the model on a target device’s HTP requires the generation of a **serialized context*</em>.*</p>
<p><strong>On the Host Machine:</strong></p>
<ol class="arabic">
<li><p>Navigate to the directory where you built the model in the previous steps:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/tmp/qnn_tmp
</pre></div>
</div>
</li>
<li><p>Users can set the custom options and different performance modes to HTP Backend through the backend config. Please refer to <a class="reference external" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/htp_backend.html">QNN HTP Backend Extensions</a> for various options available in the config.</p></li>
<li><p>Refer to the example below for creating a backend config file for the QCS6490/QCM6490 target with mandatory options passed in:
1. Update the following information based on your device’s <code class="docutils literal notranslate"><span class="pre">htp_arch</span></code>.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;graphs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;graph_names&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">                </span><span class="s2">&quot;Inception_v3&quot;</span>
<span class="w">            </span><span class="p">],</span>
<span class="w">            </span><span class="nt">&quot;vtcm_mb&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;devices&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;htp_arch&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;v68&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>The above config file with minimum parameters such as backend extensions config specified through JSON is given below:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;backend_extensions&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;shared_library_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;path_to_shared_library&quot;</span><span class="p">,</span><span class="w">  </span><span class="c1">// give path to shared extensions library (.dll)</span>
<span class="w">        </span><span class="nt">&quot;config_file_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;path_to_config_file&quot;</span><span class="w">         </span><span class="c1">// give path to backend config</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>To generate the context, update <code class="docutils literal notranslate"><span class="pre">&lt;path</span> <span class="pre">to</span> <span class="pre">JSON</span> <span class="pre">of</span> <span class="pre">backend</span> <span class="pre">extensions&gt;</span></code> below with the config you wrote above, then run the command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="p">&amp;</span><span class="w"> </span><span class="s2">&quot;</span><span class="nv">$QNN_SDK_ROOT</span><span class="s2">/bin/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/qnn-context-binary-generator&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>--backend<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/QnnHtp.dll&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>--model<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/model_libs/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/libInception_V3.so&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>--binary_file<span class="w"> </span><span class="s2">&quot;libInception_V3.serialized&quot;</span><span class="w"> </span><span class="sb">`</span>
<span class="w">    </span>--config_file<span class="w"> </span>&lt;path<span class="w"> </span>to<span class="w"> </span>JSON<span class="w"> </span>of<span class="w"> </span>backend<span class="w"> </span>extensions&gt;
</pre></div>
</div>
</li>
<li><p>This creates the serialized context at:
- <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/output/libInception_V3.serialized.bin</span></code></p></li>
</ol>
</section>
<section id="id5">
<h4>Transferring over all relevant files<a class="headerlink" href="#id5" title="Link to this heading"></a></h4>
<ol class="arabic">
<li><p>On the target device, open a terminal and make a destination folder by running:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mount<span class="w"> </span>-o<span class="w"> </span>remount,rw<span class="w"> </span>/
mkdir<span class="w"> </span>-p<span class="w"> </span>/data/local/tmp
<span class="nb">cd</span><span class="w"> </span>/data/local/tmp
ln<span class="w"> </span>-s<span class="w"> </span>/etc/<span class="w"> </span>/data/local/tmp
chmod<span class="w"> </span>-R<span class="w"> </span><span class="m">777</span><span class="w"> </span>/data/local/tmp
mkdir<span class="w"> </span>-p<span class="w"> </span>/data/local/tmp/inception_v3
</pre></div>
</div>
</li>
<li><p>Determine your target device’s SnapDragon architecture by looking your chipset up in the <a class="reference external" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/overview.html#supported-snapdragon-devices">Supported Snapdragon Devices</a> table.</p></li>
<li><p>Update the “X” values below and run the commands to set <code class="docutils literal notranslate"><span class="pre">HTP_ARCH</span></code> to match the version number found in the above table.
1. Only the 2 digits at the end should update, and they should have the same version. Ex. For “V68”, the proper value would be <code class="docutils literal notranslate"><span class="pre">hexagon-v68</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">$HTP_VERSION</span><span class="o">=</span><span class="s2">&quot;XX&quot;</span>
<span class="nv">$HTP_ARCH</span><span class="o">=</span><span class="s2">&quot;hexagon-v</span><span class="si">${</span><span class="nv">HTP_VERSION</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
</li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">scp</span></code> to transfer <code class="docutils literal notranslate"><span class="pre">libQnnHtp.so</span></code> as well as other necessary executables from your host machine to <code class="docutils literal notranslate"><span class="pre">/data/local/tmp/inception_v3</span></code> on the target device.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$QNN_SDK_ROOT</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/libQnnDsp.so&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">DSP_ARCH</span><span class="si">}</span><span class="s2">/unsigned/*&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/lib/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/libQnnDspV</span><span class="si">${</span><span class="nv">HTP_VERSION</span><span class="si">}</span><span class="s2">Stub.so&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/output/Inception_v3.serialized.bin&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Check the <a class="reference external" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/backend.html">Backend table</a> to see if there are any other processor-specific executables needed for your target processor (<code class="docutils literal notranslate"><span class="pre">DSP</span></code>) and your target device’s architecture (<code class="docutils literal notranslate"><span class="pre">$QNN_TARGET_ARCH</span></code>).
1. Use similar syntax above for <code class="docutils literal notranslate"><span class="pre">scp</span></code> to transfer any additional <code class="docutils literal notranslate"><span class="pre">.so</span></code> files listed <strong>below</strong> your selected target architecture in <a class="reference external" href="https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/backend.html">this table</a>. <strong>(There may be none!)</strong>
2. If you would like to build the model on the HTP device, ensure you also send <code class="docutils literal notranslate"><span class="pre">scp</span> <span class="pre">&quot;${QNN_SDK_ROOT}/lib/${QNN_TARGET_ARCH}/libQnnHtpPrepare.so&quot;</span> <span class="pre">&quot;/data/local/tmp/inception_v3&quot;</span></code></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Ensure you transfer the <code class="docutils literal notranslate"><span class="pre">hexagon-v##</span></code> values (in addition to the other architecture files!)</p>
</div>
</li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">scp</span></code> to transfer the example built model.
1. Update the <code class="docutils literal notranslate"><span class="pre">x64</span></code> folder below to the proper folder for your built model. The folder name depends on your host machine’s architecture.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span><span class="s2">&quot;/tmp/qnn_tmp/model_libs/x64/libInception_V3.so&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Transfer the input data, input list, and script from the QNN SDK examples folder into <code class="docutils literal notranslate"><span class="pre">/data/local/tmp/inception_v3</span></code> on the target device using <code class="docutils literal notranslate"><span class="pre">scp</span></code> in a similar way:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span>-r<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/data/cropped&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/data/target_raw_list.txt&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/data/imagenet_slim_labels&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/examples/Models/InceptionV3/scripts/show_inceptionv3_classifications.py&quot;</span><span class="w">  </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>Transfer <code class="docutils literal notranslate"><span class="pre">qnn-net-run</span></code> from <code class="docutils literal notranslate"><span class="pre">$QNN_SDK_ROOT/bin/$QNN_TARGET_ARCH/qnn-net-run</span></code> to <code class="docutils literal notranslate"><span class="pre">/data/local/tmp/inception_v3</span></code> on the target device:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span><span class="s2">/bin/</span><span class="si">${</span><span class="nv">QNN_TARGET_ARCH</span><span class="si">}</span><span class="s2">/qnn-net-run&quot;</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">:/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
<li><p>On the target device, set the environment variables:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">&quot;/data/local/tmp/inception_v3&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">ADSP_LIBRARY_PATH</span><span class="o">=</span><span class="s2">&quot;/data/local/tmp/inception_v3&quot;</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="id9">
<h4>Doing inferences on the target device processor<a class="headerlink" href="#id9" title="Link to this heading"></a></h4>
<ol class="arabic">
<li><p>Open a terminal instance on the target device.
1. Alternatively, you can <code class="docutils literal notranslate"><span class="pre">ssh</span></code> from your host machine by doing the below command.
2. These console variables were set in the above instructions for “Transferring all relevant files”.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ssh<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TARGET_USER</span><span class="si">}</span><span class="s2">@</span><span class="si">${</span><span class="nv">TARGET_IP</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You will have to log in with your target device’s login for that username.</p>
</div>
</li>
<li><p>On your target device, navigate to the directory containing the test files:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/data/local/tmp/inception_v3
</pre></div>
</div>
</li>
<li><p>Run the following command on the target device to execute an inference:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./qnn-net-run<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--backend<span class="w"> </span><span class="s2">&quot;libQnnHtp.so&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--input_list<span class="w"> </span><span class="s2">&quot;target_raw_list.txt&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--retrieve_context<span class="w"> </span><span class="s2">&quot;Inception_v3.serialized.bin&quot;</span>
<span class="w">   </span>--output<span class="w"> </span><span class="s2">&quot;./output&quot;</span>
</pre></div>
</div>
</li>
<li><p>Run the following script on the target device to view the classification results:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can alternatively copy the output folder back to your host machine with <code class="docutils literal notranslate"><span class="pre">scp</span></code> and run the following script there to avoid having to install python on your target device.</p>
</div>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span><span class="s2">&quot;.\show_inceptionv3_classifications.py&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-i<span class="w"> </span><span class="s2">&quot;.\cropped\raw_list.txt&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-o<span class="w"> </span><span class="s2">&quot;output&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-l<span class="w"> </span><span class="s2">&quot;.\imagenet_slim_labels.txt&quot;</span>
</pre></div>
</div>
</li>
<li><p>Verify that the classification results in <code class="docutils literal notranslate"><span class="pre">output</span></code> match the following:
1. <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/trash_bin.raw</span> <span class="pre">0.777344</span> <span class="pre">413</span> <span class="pre">ashcan</span></code>
2. <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/chairs.raw</span> <span class="pre">0.253906</span> <span class="pre">832</span> <span class="pre">studio</span> <span class="pre">couch</span></code>
3. <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/plastic_cup.raw</span> <span class="pre">0.980469</span> <span class="pre">648</span> <span class="pre">measuring</span> <span class="pre">cup</span></code>
4. <code class="docutils literal notranslate"><span class="pre">${QNN_SDK_ROOT}/examples/Models/InceptionV3/data/cropped/notice_sign.raw</span> <span class="pre">0.167969</span> <span class="pre">459</span> <span class="pre">brass</span></code></p></li>
</ol>
</section>
</section>
<section id="lpai">
<h3>LPAI<a class="headerlink" href="#lpai" title="Link to this heading"></a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>LPAI processors are designed for <strong>offline model preparation only</strong>. This means you must build the model first, then transfer it to the target device.</p>
</div>
<p>The offline generated model must be executed via the <a class="reference external" href="https://qpm.qualcomm.com/#/main/tools/details/LPAI">LPAI SDK</a>. You will have to sign in to access the LPAI SDK, and it may be dependent on filling out specific paperwork for your account to access the SDK.</p>
<p>You can use the following example files with the LPAI SDK to help run an inference on that processor:</p>
<p>EXAMPLE of <cite>config.json</cite> file:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;backend_extensions&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">       </span><span class="nt">&quot;shared_library_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;${QNN_SDK_ROOT}/lib/x86_64-linux-clang/libQnnLpaiNetRunExtentions.so&quot;</span><span class="p">,</span>
<span class="w">       </span><span class="nt">&quot;config_file_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./lpaiParams.conf&quot;</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>EXAMPLE of <cite>lpaiParams.conf</cite> file:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;lpai_backend&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;target_env&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;x86&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;enable_hw_ver&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;v4&quot;</span>
<span class="w">   </span><span class="p">},</span>
<span class="w">   </span><span class="nt">&quot;lpai_graph&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;prepare&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;enable_layer_fusion&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;enable_batchnorm_fold&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;enable_channel_align&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;pad_split&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;exclude_io&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
<span class="w">      </span><span class="p">},</span>
<span class="w">      </span><span class="nt">&quot;graph_name_0&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;prepare&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;enable_batchnorm_fold&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">         </span><span class="p">}</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>To configure <code class="docutils literal notranslate"><span class="pre">lpaiParams.conf</span></code>, consider using the following optional settings:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>lpai_backend
<span class="w">   </span><span class="s2">&quot;target_env&quot;</span><span class="w">              </span><span class="s2">&quot;arm/adsp/x86/tensilica, default adsp&quot;</span>
<span class="w">   </span><span class="s2">&quot;enable_hw_ver&quot;</span><span class="w">           </span><span class="s2">&quot;v1/v2/v3/v4, default v4&quot;</span>
lpai_graph
<span class="w">   </span>prepare
<span class="w">      </span><span class="s2">&quot;enable_layer_fusion&quot;</span><span class="w">     </span><span class="s2">&quot;true/false,     default true&quot;</span>
<span class="w">      </span><span class="s2">&quot;enable_batchnorm_fold&quot;</span><span class="w">   </span><span class="s2">&quot;true/false,     default false&quot;</span>
<span class="w">      </span><span class="s2">&quot;enable_channel_align&quot;</span><span class="w">    </span><span class="s2">&quot;true/false,     default false&quot;</span>
<span class="w">      </span><span class="s2">&quot;pad_split&quot;</span><span class="w">               </span><span class="s2">&quot;true/false,     default false&quot;</span>
<span class="w">      </span><span class="s2">&quot;exclude_io&quot;</span><span class="w">              </span><span class="s2">&quot;true/false,     default false&quot;</span>
</pre></div>
</div>
<p>Using the above <code class="docutils literal notranslate"><span class="pre">config.json</span></code> and <code class="docutils literal notranslate"><span class="pre">lpaiParams.conf</span></code> you can use <code class="docutils literal notranslate"><span class="pre">qnn-context-binary-generator</span></code> to build the LPAI offline model. Replace every value wrapped in triangle brackets (&lt;&gt;).</p>
<p>When files are mentioned, ensure that they have the relative or absolute path to that value.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/examples/QNN/converter/models<span class="w"> </span><span class="sb">`</span>
<span class="p">&amp;</span><span class="w"> </span><span class="nv">$LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:<span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/lib/x86_64-linux-clang<span class="w"> </span><span class="sb">`</span>
<span class="p">&amp;</span><span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/bin/x86_64-linux-clang/qnn-context-binary-generator<span class="w"> </span><span class="sb">`</span>
<span class="w">              </span>--backend<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/lib/x86_64-linux-clang/libQnnLpai.so<span class="w"> </span><span class="sb">`</span>
<span class="w">              </span>--model<span class="w"> </span><span class="si">${</span><span class="nv">QNN_SDK_ROOT</span><span class="si">}</span>/examples/Models/InceptionV3/model_libs/x86_64-linux-clang/&lt;libQnnModel.so&gt;<span class="w"> </span><span class="sb">`</span>
<span class="w">              </span>--config_file<span class="w"> </span>&lt;config.json&gt;<span class="w"> </span><span class="sb">`</span>
<span class="w">              </span>--log_level<span class="w"> </span>verbose<span class="w"> </span><span class="sb">`</span>
<span class="w">              </span>--backend_binary<span class="w"> </span>&lt;output_graph.eai&gt;<span class="w"> </span><span class="sb">`</span>
<span class="w">              </span>--binary_file<span class="w"> </span>tmp
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="qnn_tutorial_windows_host.html" class="btn btn-neutral float-left" title="CNN to QNN for Windows Host" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../setup.html" class="btn btn-neutral float-right" title="Setup" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2024, Qualcomm Technologies, Inc..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>